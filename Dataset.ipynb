{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy, string\n",
    "from spacy.vocab import Vocab\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "        \n",
    "class Dataset:\n",
    "\n",
    "    # dataset initialization\n",
    "    def __init__(self, path):\n",
    "        \n",
    "        # sets the file name i.e. type:\n",
    "        self.path = path\n",
    "        \n",
    "        # loads input data\n",
    "        self.data = nlp(open(path + '.csv').read())\n",
    "        \n",
    "        # initializes the vocabulary size:\n",
    "        self.vocabulary_size = 0\n",
    "        \n",
    "        print('init for ' + self.path + ' is done')\n",
    "\n",
    "    # returns the age group of the dataset or the file name\n",
    "    def get_path(self):\n",
    "        return self.path\n",
    "    \n",
    "    # returns the raw dataset    \n",
    "    def get_data(self):\n",
    "        return self.data \n",
    "    \n",
    "    # returns the vocabulary size:\n",
    "    def get_vocabulary_size(self):\n",
    "        return self.vocabulary_size\n",
    "\n",
    "    # CALCULATE THE NR. OF CHARACTERS IN THE INPUT DOCUMENT\n",
    "    def calculate_length (self):\n",
    "        total_length = 0\n",
    "\n",
    "        for token in self.data:\n",
    "            total_length = total_length + len(token.text)\n",
    "    \n",
    "        return total_length\n",
    "    \n",
    "    # CREATE VOCABULARY\n",
    "    def create_vocabulary(self, save):\n",
    "        bag_of_words = set()\n",
    "\n",
    "        # create bag of words and save copy\n",
    "        for token in self.data:\n",
    "            if (token.text not in bag_of_words):\n",
    "                bag_of_words.add(token.text)\n",
    "                self.vocabulary_size += 1\n",
    "\n",
    "        if (save):\n",
    "            with open('voc_' + self.path + '.csv', 'w') as voc:\n",
    "                for word in bag_of_words:\n",
    "                    voc.write(word + ' ') \n",
    "                \n",
    "        print('vocabulary for ' + self.path + ' is created, size: ' + str(self.vocabulary_size))\n",
    "        return bag_of_words\n",
    "    \n",
    "    # CREATE DICTIONARY\n",
    "    def create_dictionary(self, bag_of_words, save):\n",
    "        # create dictionary from all words\n",
    "        dictionary = dict.fromkeys(bag_of_words, 0)\n",
    "\n",
    "        # count words in bag\n",
    "        for token in self.data:\n",
    "            dictionary[token.text] += 1    \n",
    "\n",
    "        # save copy\n",
    "        if (save):\n",
    "            with open('dict_' + self.path + '.csv', 'w') as doc_out:\n",
    "                for word, freq in dictionary.items():\n",
    "                    doc_out.write(word + ': ' + str(freq) + '\\n')   \n",
    "                \n",
    "        print('dictionary for ' + self.path + ' is created')\n",
    "        return dictionary\n",
    "\n",
    "    \n",
    "    # COUNT WORD FREQUENCY\n",
    "    def calc_termFrequency(self, dictionary):\n",
    "        tfDict = {}\n",
    "        count_in_data = len(self.data)\n",
    "\n",
    "        for word, count_in_dict in dictionary.items():\n",
    "            tfDict[word] = count_in_dict / float(count_in_data)\n",
    "\n",
    "        print('term frequencies for ' + self.path + ' are generated')\n",
    "        return tfDict     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNERS FOR GROUP LEVEL ANALYSIS\n",
    "save = True\n",
    "paths = ['./IO_YO/young', './IO_YO/old']\n",
    "doc_types = ['_in', '_norm']\n",
    "\n",
    "# EXECUTION\n",
    "dataset_y = Dataset(paths[0] + doc_types[1])\n",
    "dataset_o = Dataset(paths[1] + doc_types[1])\n",
    "\n",
    "#tfDict = dataset.calc_termFrequency(dict)\n",
    "#dict_o = dataset_o.create_dictionary(dataset_o.create_vocabulary(save), save)\n",
    "#dict_y = dataset_y.create_dictionary(dataset_y.create_vocabulary(save), save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init for Swiss_EAR_young is done\n",
      "init for Swiss_EAR_young is done\n"
     ]
    }
   ],
   "source": [
    "# TUNERS FOR PARTICIPANT LEVEL ANALYSIS\n",
    "save = False\n",
    "participant_ids = nlp(open('PARTICIPANT_ID.txt').read())\n",
    "paths = ['./IO_P/' + token.text for token in participant_ids]\n",
    "doc_types = ['_in', '_norm']\n",
    "\n",
    "#EXECUTION\n",
    "for path in paths:\n",
    "    if path is not './IO_P/\\n':\n",
    "        dataset = Dataset(path + doc_types[1])\n",
    "        dataset.create_vocabulary(save)\n",
    "        voc_size.append(word + ': ' + str(dataset.get_vocabulary_size()) + '\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### TUNER\n",
    "save = 'False' #NE ÁLLÍTSD ÁT, MERT 330 FÁJLT FOG ÍRNI\n",
    "\n",
    "ids= nlp(open('PARTICIPANT_ID.txt').read())\n",
    "ids = [token.text for token in ids]\n",
    "doc_type = ['in', 'norm']\n",
    "\n",
    "for word in ids:\n",
    "    with open(word + 'in' + '.csv', 'w') as data_creator:\n",
    "        data = nlp(open(word + '.csv').read())\n",
    "        data_str = [token.text for token in data]\n",
    "        for element in data_str:\n",
    "            if element == '\\n':\n",
    "                data_creator.write(element) \n",
    "            else:\n",
    "                data_creator.write(element + ' ') \n",
    "    print(word + ' done')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
