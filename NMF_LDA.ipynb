{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANEV\\Anaconda3\\lib\\site-packages\\past\\types\\oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "C:\\Users\\ANEV\\Anaconda3\\lib\\site-packages\\past\\builtins\\misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n"
     ]
    }
   ],
   "source": [
    "# IMPORT PACKAGES\n",
    "import spacy, pandas, numpy, string\n",
    "from spacy.lang.de import German\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA S.T. 1 LINE IN XLSX = 1 DOCUMENT\n",
    "def load_data (path):\n",
    "    data_raw = open(path + '.csv', encoding = 'utf-8').read().replace('\\\"', '').replace('\\ufeff', '')\n",
    "    data_list = data_raw.split('\\n')\n",
    "    \n",
    "    data_list_remove_empty_last_line = []\n",
    "    for row in range(0, len(data_list)-1):\n",
    "        data_list_remove_empty_last_line.append(data_list[row])\n",
    "    \n",
    "    input_table = [row.split(';') for row in data_list_remove_empty_last_line]\n",
    "    \n",
    "    print(path + '.csv: data loaded')\n",
    "    return data_list_remove_empty_last_line, input_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE TF / IDF / TF-IDF VALUES\n",
    "def vectorize (vectorizer_type, min_df, max_df):\n",
    "    print(vectorizer_type)\n",
    "    \n",
    "    if vectorizer_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(max_df = max_df,\n",
    "                                     min_df = min_df)\n",
    "        print('Tfidf-vectorizing done')\n",
    "        return vectorizer\n",
    "    \n",
    "    elif vectorizer_type == 'tf':\n",
    "        vectorizer = CountVectorizer(max_df = max_df,\n",
    "                                     min_df = min_df)\n",
    "        print('Count-vectorizing done')\n",
    "        return vectorizer                 \n",
    "            \n",
    "    else:\n",
    "        print('error: unknown vectorizer')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE SPARSE DOC-TERM MATRIX \n",
    "def generate_input_matrix(vectorizer, input_table):\n",
    "    t = time()\n",
    "\n",
    "    column1 = [row[0] for row in input_table] \n",
    "    doc_term_matrix = vectorizer.fit_transform(column1)\n",
    "    \n",
    "    print('doc-term matrix generated in %s s' %round((time() - t), 5) + ', matrix dimensions: ' + str(doc_term_matrix.shape))\n",
    "    return doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF, Source: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "def generate_nmf_topic_model(doc_term_matrix, beta_loss, n_topics, max_iterations):\n",
    "    t = time()\n",
    "\n",
    "    # NMF - Frobenius-norm : ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2\n",
    "    # math: d_{Fro}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2\n",
    "    if beta_loss == 'frobenius':\n",
    "        matrix_factorization = NMF(beta_loss = beta_loss, n_components = n_topics, max_iter = max_iterations)\n",
    "        nmf_model = matrix_factorization.fit(doc_term_matrix)\n",
    "        print('NMF Frobenius topic_model created in %s s' %round((time() - t), 5))\n",
    "\n",
    "    # NMF - Kullback-Leibler divergence:\n",
    "    # math: d_{KL}(X, Y) = \\sum_{i,j} (X_{ij} log(\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})\n",
    "    elif beta_loss == 'kullback-leibler':\n",
    "        matrix_factorization = NMF(beta_loss = beta_loss, n_components = n_topics, max_iter = max_iterations, solver = 'mu')\n",
    "        nmf_model = matrix_factorization.fit(doc_term_matrix)\n",
    "        print('NMF Kullback-Leibler topic_model created in %s s' %round((time() - t), 5))           \n",
    "    \n",
    "    else:\n",
    "        print('error: invalid beta_loss')\n",
    "        \n",
    "    return matrix_factorization, nmf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA, Source: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "#def generate_lda_topic_model(doc_topic_prior, doc_term_matrix, n_topics, max_iterations, learning_method, learning_offset):\n",
    "def generate_lda_topic_model(doc_term_matrix, n_topics, max_iterations, learning_method, learning_offset):    \n",
    "    t = time()\n",
    "    matrix_factorization = LatentDirichletAllocation(n_components = n_topics,\n",
    "                                                #    doc_topic_prior = doc_topic_prior,\n",
    "                                                #    topic_word_prior = topic_word_prior, \n",
    "                                                    learning_method = learning_method,\n",
    "                                                    learning_offset = learning_offset, \n",
    "                                                    max_iter = max_iterations)\n",
    "    \n",
    "    lda_model = matrix_factorization.fit(doc_term_matrix)\n",
    "    print('LDA topic_model created in %s s' %round((time() - t), 5))     \n",
    "    \n",
    "    return matrix_factorization, lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT TOPICS, Source: https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "def print_topics (path, topic_model, feature_names, n_top_words, save):\n",
    "\n",
    "    # calculates the top words of topics                        \n",
    "    if (save):\n",
    "        with open(path + '_topics.csv', 'w', encoding = 'latin-1') as doc_out:\n",
    "            for idx, topic in enumerate(topic_model.components_):\n",
    "                topic_list = \"#%d, \" % (idx + 1)\n",
    "                topic_list += \" \".join([\"{} ({}),\".format(feature_names[idx], str(round(topic[idx], 3))) for idx in topic.argsort()[:-n_top_words - 1:-1]]) \n",
    "                doc_out.write(topic_list + '\\n')\n",
    "                \n",
    "    else:\n",
    "        for idx, topic in enumerate(topic_model.components_):\n",
    "            topic_list = \"Topic #%d: \" % (idx + 1)\n",
    "            topic_list += \" \".join([\"{} ({}),\".format(feature_names[idx], str(round(topic[idx], 3))) for idx in topic.argsort()[:-n_top_words - 1:-1]]) \n",
    "            print(topic_list)\n",
    "\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE AND PRINT TOPIC DISTRIBUTION\n",
    "def calculate_topic_distribution(path, topic_model, doc_term_matrix, data_list, feature_names): \n",
    "\n",
    "    # TRANSFORM DATA INTO DATAFRAME  \n",
    "    row_index = 1\n",
    "    rows = []\n",
    "    for row in data_list:\n",
    "        rows.append(str(row_index) + ';' + row)\n",
    "        row_index = row_index + 1\n",
    "    \n",
    "    columns = [\"Topic #%d: \" % (index + 1) for index, topic in enumerate(topic_model.components_)]\n",
    "    values = numpy.round(topic_model.transform(doc_term_matrix), 2)\n",
    "    df = pandas.DataFrame(values, rows, columns)\n",
    "    df.index.names = ['Row;Record;ID;Age;Date;W;Partner;Family;Friend;Stranger;Past;Future']\n",
    "\n",
    "    # CALCULATE DOMINANT TOPIC / DOC\n",
    "    dominant_topic = numpy.argmax(df.values, axis = 1) + 1  \n",
    "    df['dominant_topic'] =   dominant_topic\n",
    "#    df = df.sort_values('dominant_topic')\n",
    "\n",
    "#    topic_total = numpy.sum(values, axis = 0)\n",
    "#    df.loc['total:', columns] = topic_total\n",
    "#    df.loc['total:', dominant_topic] = numpy.argmax(topic_total, 0) + 1\n",
    "    \n",
    "    # PRINT TO CSV\n",
    "    df.to_csv(path + '_distr.csv', sep=';', columns=None, header=True, index=True, index_label=None, mode='w', encoding='utf-8', compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.')\n",
    "    \n",
    "    print('topic distribution is done and printed')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./IO_YO/all_2Snorm.csv: data loaded\n",
      "number of topics in current iteration: 3\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06782 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 87.76807 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 4\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.08996 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 94.5638 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 5\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05059 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 91.17317 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 6\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.12012 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 96.82005 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 7\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.04848 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 99.59274 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 8\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06029 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 98.79985 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 9\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.09039 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 114.5578 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 10\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.12594 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 126.73426 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 11\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05428 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 117.72297 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 12\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05688 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 125.88896 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 13\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05588 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 128.10201 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 14\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.10982 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 162.51062 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 15\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05487 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 128.8378 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 16\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05498 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 147.10504 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 17\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.055 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 158.03433 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 18\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.0619 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 159.40938 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 19\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.08486 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 184.66798 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 20\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.08178 s, matrix dimensions: (5769, 7061)\n",
      "LDA topic_model created in 171.7425 s\n",
      "topic distribution is done and printed\n",
      "lda done\n"
     ]
    }
   ],
   "source": [
    "# TUNERS\n",
    "n_top_words = 5\n",
    "max_iterations = 200\n",
    "save = True\n",
    "# -----------------------------------------------------------\n",
    "#doc_types = ['1norm', '1Snorm', '2norm', '2Snorm']\n",
    "\n",
    "data_list, input_table = load_data('./IO_YO/all_2Snorm')\n",
    "\n",
    "for i in range(3, 21):\n",
    "    n_topics = i\n",
    "    min_df = 1\n",
    "    max_df = 90\n",
    "    model_specification_in_file_name = '_min' + str(min_df) + '_max' + str(max_df) + '_' + str(n_topics)\n",
    "    print('number of topics in current iteration: ' + str(i))\n",
    "    \n",
    "    tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "    tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "    \n",
    "#    tfidf_vectorizer = vectorize ('tfidf', min_df, max_df)\n",
    "#    tfidf_doc_term_matrix = generate_input_matrix (tfidf_vectorizer, input_table)\n",
    "\n",
    "# PREPARE LDA TOPIC MODEL\n",
    "    path = './IO_YO/lda' + model_specification_in_file_name\n",
    "#    matrix_factorization, topic_model = generate_lda_topic_model (1.0, tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\n",
    "    matrix_factorization, topic_model = generate_lda_topic_model (tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0   \n",
    "    topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())\n",
    "    print('lda done')\n",
    "    \"\"\"\n",
    "# PREPARE NMF TOPIC MODEL\n",
    "    path = './IO_YO/nmf_fr' + model_specification_in_file_name\n",
    "    matrix_factorization, topic_model = generate_nmf_topic_model (tfidf_doc_term_matrix, 'frobenius', n_topics, max_iterations)\n",
    "    topic_list = print_topics (path, topic_model, tfidf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tfidf_doc_term_matrix, data_list, tfidf_vectorizer.get_feature_names())\n",
    "    print('nmf_frob done')\n",
    "\n",
    "# PREPARE NMF TOPIC MODEL\n",
    "    path = './IO_YO/nmf_kl' + model_specification_in_file_name\n",
    "    matrix_factorization, topic_model = generate_nmf_topic_model (tfidf_doc_term_matrix, 'kullback-leibler', n_topics, max_iterations)\n",
    "    topic_list = print_topics (path, topic_model, tfidf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tfidf_doc_term_matrix, data_list, tfidf_vectorizer.get_feature_names())\n",
    "    print('nmf_kl done')\n",
    "    \n",
    "# PREPARE NMF TOPIC MODEL\n",
    "    path = './IO_YO/nmf_tf' + model_specification_in_file_name\n",
    "    matrix_factorization, topic_model = generate_nmf_topic_model (tf_doc_term_matrix, 'frobenius', n_topics, max_iterations)\n",
    "    topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())    \n",
    "    print('nmf with TF done')    \n",
    "    \"\"\"\n",
    "#pyLDAvis.enable_notebook()\n",
    "#pyLDAvis.sklearn.prepare(matrix_factorization, doc_term_matrix, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics in current iteration: 3\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.09374 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 73.62526 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 4\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06025 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 72.35988 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 5\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06016 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 73.62006 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 6\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06538 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 81.7235 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 7\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05984 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 79.45547 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 8\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.13493 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 65.7749 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 9\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05981 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 78.31047 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 10\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05991 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 77.16517 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 11\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06052 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 77.45959 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 12\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.13045 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 68.35786 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 13\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06604 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 72.5443 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 14\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05008 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 77.27789 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 15\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.0506 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 78.14054 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 16\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05839 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 144.22429 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 17\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06051 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 85.96909 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 18\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.1024 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 76.14322 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 19\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.10137 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 76.79372 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 20\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.09106 s, matrix dimensions: (5769, 2433)\n",
      "LDA topic_model created in 82.21099 s\n",
      "topic distribution is done and printed\n",
      "lda done\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 21):\n",
    "    n_topics = i\n",
    "    min_df = 2\n",
    "    max_df = 90\n",
    "    model_specification_in_file_name = '_min' + str(min_df) + '_max' + str(max_df) + '_' + str(n_topics)\n",
    "    print('number of topics in current iteration: ' + str(i))\n",
    "    \n",
    "    tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "    tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "    \n",
    "# PREPARE LDA TOPIC MODEL\n",
    "    path = './IO_YO/lda' + model_specification_in_file_name\n",
    "#    matrix_factorization, topic_model = generate_lda_topic_model (1.0, tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\n",
    "    matrix_factorization, topic_model = generate_lda_topic_model (tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0   \n",
    "    topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())\n",
    "    print('lda done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics in current iteration: 3\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.10024 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 70.91324 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 4\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.07296 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 73.36167 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 5\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.07099 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 72.35954 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 6\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.0607 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 93.86444 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 7\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05848 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 102.63813 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 8\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05692 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 102.95601 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 9\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05254 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 108.15269 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 10\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.04686 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 113.24143 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 11\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05868 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 117.87642 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 12\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06248 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 115.44813 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 13\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05857 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 116.04451 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 14\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05043 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 121.32605 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 15\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05043 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 143.87663 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 16\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06101 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 150.28019 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 17\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.07137 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 157.10541 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 18\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.069 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 155.79952 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 19\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.07891 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 187.14629 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 20\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.08468 s, matrix dimensions: (5769, 7058)\n",
      "LDA topic_model created in 193.68184 s\n",
      "topic distribution is done and printed\n",
      "lda done\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 21):\n",
    "    n_topics = i\n",
    "    min_df = 1\n",
    "    max_df = 85\n",
    "    model_specification_in_file_name = '_min' + str(min_df) + '_max' + str(max_df) + '_' + str(n_topics)\n",
    "    print('number of topics in current iteration: ' + str(i))\n",
    "    \n",
    "    tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "    tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "    \n",
    "# PREPARE LDA TOPIC MODEL\n",
    "    path = './IO_YO/lda' + model_specification_in_file_name\n",
    "#    matrix_factorization, topic_model = generate_lda_topic_model (1.0, tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\n",
    "    matrix_factorization, topic_model = generate_lda_topic_model (tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0   \n",
    "    topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())\n",
    "    print('lda done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics in current iteration: 3\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.07909 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 78.282 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 4\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.08471 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 77.00756 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 5\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06116 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 69.19299 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 6\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06904 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 57.88906 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 7\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.0534 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 56.6928 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 8\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05687 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 57.09056 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 9\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06064 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 55.92893 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 10\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.04844 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 57.15757 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 11\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.04853 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 64.29358 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 12\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.0534 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 64.11641 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 13\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.0469 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 65.12467 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 14\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05242 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 64.98649 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 15\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.0504 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 65.78007 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 16\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.04834 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 77.63814 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 17\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05866 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 76.5589 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 18\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.05839 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 78.77924 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 19\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06038 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 81.3368 s\n",
      "topic distribution is done and printed\n",
      "lda done\n",
      "number of topics in current iteration: 20\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.07836 s, matrix dimensions: (5769, 2430)\n",
      "LDA topic_model created in 79.91865 s\n",
      "topic distribution is done and printed\n",
      "lda done\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 21):\n",
    "    n_topics = i\n",
    "    min_df = 2\n",
    "    max_df = 85\n",
    "    model_specification_in_file_name = '_min' + str(min_df) + '_max' + str(max_df) + '_' + str(n_topics)\n",
    "    print('number of topics in current iteration: ' + str(i))\n",
    "    \n",
    "    tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "    tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "    \n",
    "# PREPARE LDA TOPIC MODEL\n",
    "    path = './IO_YO/lda' + model_specification_in_file_name\n",
    "#    matrix_factorization, topic_model = generate_lda_topic_model (1.0, tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\n",
    "    matrix_factorization, topic_model = generate_lda_topic_model (tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0   \n",
    "    topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())\n",
    "    print('lda done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of topics in current iteration: 3\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.06058 s, matrix dimensions: (5769, 7056)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_lda_topic_model() got multiple values for argument 'learning_method'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-554916505c6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# PREPARE LDA TOPIC MODEL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./IO_YO/lda'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel_specification_in_file_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmatrix_factorization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_lda_topic_model\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_doc_term_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'online'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50.\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, random_state = 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m#    matrix_factorization, topic_model = generate_lda_topic_model (tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mtopic_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprint_topics\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_lda_topic_model() got multiple values for argument 'learning_method'"
     ]
    }
   ],
   "source": [
    "for i in range(3, 21):\n",
    "    n_topics = i\n",
    "    min_df = 1\n",
    "    max_df = 80\n",
    "    model_specification_in_file_name = '_min' + str(min_df) + '_max' + str(max_df) + '_' + str(n_topics)\n",
    "    print('number of topics in current iteration: ' + str(i))\n",
    "    \n",
    "    tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "    tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "\n",
    "# PREPARE LDA TOPIC MODEL\n",
    "    path = './IO_YO/lda' + model_specification_in_file_name\n",
    "    matrix_factorization, topic_model = generate_lda_topic_model (1.0, tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\n",
    "#    matrix_factorization, topic_model = generate_lda_topic_model (tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0   \n",
    "    topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())\n",
    "    print('lda done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 21):\n",
    "    n_topics = i\n",
    "    min_df = 2\n",
    "    max_df = 80\n",
    "    model_specification_in_file_name = '_min' + str(min_df) + '_max' + str(max_df) + '_' + str(n_topics)\n",
    "    print('number of topics in current iteration: ' + str(i))\n",
    "    \n",
    "    tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "    tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "\n",
    "# PREPARE LDA TOPIC MODEL\n",
    "    path = './IO_YO/lda' + model_specification_in_file_name\n",
    "#    matrix_factorization, topic_model = generate_lda_topic_model (1.0, tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\n",
    "    matrix_factorization, topic_model = generate_lda_topic_model (tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0   \n",
    "    topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())\n",
    "    print('lda done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# TUNERS\n",
    "n_top_words = 5\n",
    "max_iterations = 200\n",
    "save = True\n",
    "min_df = 2\n",
    "max_df = 85\n",
    "# -----------------------------------------------------------\n",
    "data_list, input_table = load_data('./IO_YO/all_2Snorm')\n",
    "\n",
    "# ALPHA 0.5\n",
    "for i in range(3, 21):\n",
    "    n_topics = i\n",
    "    alpha = 0.5\n",
    "    beta = None\n",
    "    model_specification_in_file_name = '_min' + str(min_df) + '_max' + str(max_df) + '_alfa' + str(alpha) + '_' + str(n_topics)\n",
    "    print('number of topics in current iteration: ' + str(i))\n",
    "    \n",
    "    tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "    tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "\n",
    "    path = './IO_YO/topic_lda' + model_specification_in_file_name\n",
    "    matrix_factorization, topic_model = generate_lda_topic_model (alpha, tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\n",
    "    topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names()) \n",
    "\n",
    "# ALPHA 1.0\n",
    "for i in range(3, 21):\n",
    "    n_topics = i\n",
    "    alpha = 1.0\n",
    "    beta = None\n",
    "    model_specification_in_file_name = '_min' + str(min_df) + '_max' + str(max_df) + '_alfa' + str(alpha) + '_' + str(n_topics)\n",
    "    print('number of topics in current iteration: ' + str(i))\n",
    "    \n",
    "    tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "    tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "\n",
    "    path = './IO_YO/topic_lda' + model_specification_in_file_name\n",
    "    matrix_factorization, topic_model = generate_lda_topic_model (alpha, tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\n",
    "    topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())\n",
    "\n",
    "# ALPHA 1.5\n",
    "for i in range(3, 21):\n",
    "    n_topics = i\n",
    "    alpha = 1.5\n",
    "    beta = None\n",
    "    model_specification_in_file_name = '_min' + str(min_df) + '_max' + str(max_df) + '_alfa' + str(alpha) + '_' + str(n_topics)\n",
    "    print('number of topics in current iteration: ' + str(i))\n",
    "    \n",
    "    tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "    tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "\n",
    "    path = './IO_YO/topic_lda' + model_specification_in_file_name\n",
    "    matrix_factorization, topic_model = generate_lda_topic_model (alpha, tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\n",
    "    topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())    \n",
    "\n",
    "# ALPHA 2.0\n",
    "for i in range(3, 21):\n",
    "    n_topics = i\n",
    "    alpha = 2.0\n",
    "    beta = None\n",
    "    model_specification_in_file_name = '_min' + str(min_df) + '_max' + str(max_df) + '_alfa' + str(alpha) + '_' + str(n_topics)\n",
    "    print('number of topics in current iteration: ' + str(i))\n",
    "    \n",
    "    tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "    tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "\n",
    "# PREPARE LDA TOPIC MODEL\n",
    "    path = './IO_YO/topic_lda' + model_specification_in_file_name\n",
    "    matrix_factorization, topic_model = generate_lda_topic_model (alpha, tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\n",
    "    topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "    df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())     \n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
