{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "import spacy, pandas, numpy\n",
    "from spacy.lang.de import German #to create spacy parser\n",
    "#nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "#from sklearn.pipeline import Pipeline #process gets faster, + for the method calls\n",
    "\n",
    "# source: https://docs.python.org/3/library/time.html\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA S.T. 1 LINE IN XLSX = 1 DOCUMENT\n",
    "def load_data(path):\n",
    "#    t = time()\n",
    "    with open(path + '.csv', 'r', encoding = 'latin-1') as doc:\n",
    "        data = [line for line in doc] # string list\n",
    "\n",
    "    print(path + '.csv: data loaded')   \n",
    "#    print('data loaded in %s ms' %(1000 * round((time() - t), 5)))\n",
    "          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE TF / IDF / TF-IDF VALUES\n",
    "def vectorizer (vectorizer_type, min_df, max_df):\n",
    "    print(vectorizer_type)\n",
    "    \n",
    "    if vectorizer_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(max_df = max_df, #ignore terms that appear more than in...\n",
    "                                     min_df = min_df) #ignore terms that appear less than in...\n",
    "        print('Tfidf-vectorizing done')\n",
    "        return vectorizer\n",
    "    \n",
    "    elif vectorizer_type == 'tf':\n",
    "        vectorizer = CountVectorizer(max_df = max_df,\n",
    "                                     min_df = min_df)            \n",
    "        print('Count-vectorizing done')\n",
    "        return vectorizer                 \n",
    "            \n",
    "    else:\n",
    "        print('error: unknown vectorizer')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DOC-TERM MATRIX \n",
    "def generate_input_matrix(vectorizer, data):\n",
    "    t = time()\n",
    "    doc_term_matrix = vectorizer.fit_transform(data)\n",
    "    print('doc-term matrix generated in %s s' %round((time() - t), 5) + ', matrix dimensions: ' + str(doc_term_matrix.shape))\n",
    "    \n",
    "    return doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF: Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X\n",
    "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "def generate_nmf_topic_model(doc_term_matrix, beta_loss, n_topics, max_iterations):\n",
    "    t = time()\n",
    "\n",
    "    # NMF - Frobenius-norm : ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2\n",
    "    # math: d_{Fro}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2\n",
    "    if beta_loss == 'frobenius':\n",
    "        matrix_factorization = NMF(beta_loss = beta_loss, n_components = n_topics, max_iter = max_iterations)\n",
    "        nmf_model = matrix_factorization.fit(doc_term_matrix)\n",
    "        print('NMF Frobenius topic_model created in %s s' %round((time() - t), 5))\n",
    "\n",
    "    # NMF - Kullback-Leibler divergence:\n",
    "    # math: d_{KL}(X, Y) = \\sum_{i,j} (X_{ij} log(\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})\n",
    "    elif beta_loss == 'kullback-leibler':\n",
    "        matrix_factorization = NMF(beta_loss = beta_loss, n_components = n_topics, max_iter = max_iterations, solver = 'mu')\n",
    "        nmf_model = matrix_factorization.fit(doc_term_matrix)\n",
    "        print('NMF Kullback-Leibler topic_model created in %s s' %round((time() - t), 5))        \n",
    "        \n",
    "    # NMF - Itakura-Saito divergence:    \n",
    "    # math. d_{IS}(X, Y) = \\sum_{i,j} (\\frac{X_{ij}}{Y_{ij}} - log(\\frac{X_{ij}}{Y_{ij}}) - 1)  \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        print('error: invalid beta_loss')\n",
    "        \n",
    "    return nmf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "def generate_lda_topic_model(doc_term_matrix, n_topics, max_iter):\n",
    "    t = time()\n",
    "    matrix_factorization = LatentDirichletAllocation(n_components =  n_topics,\n",
    "                                                    max_iter = max_iter)\n",
    "    \n",
    "    lda_model = matrix_factorization.fit(doc_term_matrix)\n",
    "    print('LDA topic_model created in %s s' %round((time() - t), 5))     \n",
    "    \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT TOPICS TO CONSOLE AND/OR SAVE TOPICS TO FILE\n",
    "# Source: https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "def visualize_topics (path, model, model_type, feature_names, n_topics, n_top_words, save):\n",
    "    # calculates the top words of topics                    \n",
    "    if (save):\n",
    "        with open(path + '_tm_' + model_type + '_' + str(n_topics) + '_' + str(n_top_words) + '.csv', 'w', encoding = 'latin-1') as doc_out:\n",
    "            for idx, topic in enumerate(model.components_):\n",
    "                topic_list = \"Topic #%d: \" % (idx + 1)\n",
    "                topic_list += \" \".join([\"{} ({}),\".format(feature_names[idx], str(round(topic[idx], 3))) for idx in topic.argsort()[:-n_top_words - 1:-1]]) \n",
    "                doc_out.write(topic_list + '\\n')\n",
    "                \n",
    "    else:\n",
    "        for idx, topic in enumerate(model.components_):\n",
    "            topic_list = \"Topic #%d: \" % (idx + 1)\n",
    "            topic_list += \" \".join([\"{} ({}),\".format(feature_names[idx], str(round(topic[idx], 3))) for idx in topic.argsort()[:-n_top_words - 1:-1]]) \n",
    "            print(topic_list)\n",
    "\n",
    "    return topic_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./IO_YO/old_norm.csv: data loaded\n",
      "tf\n",
      "Count-vectorizing done\n",
      "doc-term matrix generated in 0.03988 s, matrix dimensions: (2140, 4927)\n",
      "LDA topic_model created in 48.44751 s\n"
     ]
    }
   ],
   "source": [
    "# TUNERS\n",
    "vectorizer_types = ['tf', 'tfidf']\n",
    "model_types = ['frobenius', 'kullback-leibler', 'itakura-saito']\n",
    "vectorizer_type = vectorizer_types[0]\n",
    "model_type = 'nmf'\n",
    "n_topics = 40\n",
    "max_iterations = 200\n",
    "n_top_words = 5\n",
    "save = True\n",
    "\n",
    "# tuners for group level analysis\n",
    "paths = ['./IO_YO/young', './IO_YO/old']\n",
    "doc_type = ['_tm_xy_c', '_tm_norm', '_norm']\n",
    "\n",
    "# tuners for person level analysis\n",
    "#participant_ids = nlp(open('PARTICIPANT_ID.txt').read())\n",
    "#paths = ['./IO_P/' + token.text for token in participant_ids]\n",
    "\n",
    "# EXECUTE\n",
    "# load data\n",
    "path = './IO_YO/old'\n",
    "data = load_data(path + doc_type[2])\n",
    "\n",
    "if (model_type in model_types):\n",
    "    # choose vectorizer     min_df = 2 ignorálja a csak 1x elôforduló szavakat, max_df = 1.0 alap\n",
    "#    vectorizer = vectorizer('tfidf', min_df = 2, max_df = 1.0)\n",
    "    vectorizer = vectorizer('tfidf', min_df = 1, max_df = 1.0)\n",
    "    # generate input matrix\n",
    "    doc_term_matrix = generate_input_matrix(vectorizer, data)\n",
    "    # do black magic\n",
    "    topic_model = generate_nmf_topic_model(doc_term_matrix, model_type, n_topics, max_iterations)\n",
    "    # print topics\n",
    "    topic_list = visualize_topics (path, topic_model, model_type, vectorizer.get_feature_names(), n_topics, n_top_words, save)\n",
    "    \n",
    "else:\n",
    "    # choose vectorizer    min_df = 2 ignorálja a csak 1x elôforduló szavakat, max_df = 1.0 alap\n",
    "#    vectorizer = vectorizer('tf', min_df = 2, max_df = 1.0)\n",
    "    vectorizer = vectorizer('tf', min_df = 1, max_df = 1.0)\n",
    "    # generate input matrix\n",
    "    doc_term_matrix = generate_input_matrix(vectorizer, data)\n",
    "    # do black magic\n",
    "    topic_model = generate_lda_topic_model(doc_term_matrix, n_topics, max_iterations) \n",
    "    # print topics\n",
    "    topic_list = visualize_topics (path, topic_model, model_type, vectorizer.get_feature_names(), n_topics, n_top_words, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# EXECUTE\\n# load data\\nfor path in paths:\\n    data = load_data(path + doc_type[0])\\n\\n    if (model_type in model_types):\\n        # choose vectorizer     min_df = 2 ignorálja a csak 1x elôforduló szavakat, max_df = 1.0 alap\\n#        vectorizer = vectorizer('tfidf', min_df = 2, max_df = 1.0)\\n        vectorizer = vectorizer('tfidf', min_df = 1, max_df = 1.0)\\n        # generate input matrix\\n        doc_term_matrix = generate_input_matrix(vectorizer, data)\\n        # do black magic\\n        topic_model = generate_nmf_topic_model(doc_term_matrix, model_type, n_topics, max_iterations)\\n        # print topics\\n        topic_list = visualize_topics (path, topic_model, model_type, vectorizer.get_feature_names(), n_topics, n_top_words, save)\\n    \\n    else:\\n        # choose vectorizer    min_df = 2 ignorálja a csak 1x elôforduló szavakat, max_df = 1.0 alap\\n#        vectorizer = vectorizer('tf', min_df = 2, max_df = 1.0)\\n        vectorizer = vectorizer('tf', min_df = 1, max_df = 1.0)\\n        # generate input matrix\\n        doc_term_matrix = generate_input_matrix(vectorizer, data)\\n        # do black magic\\n        topic_model = generate_lda_topic_model(doc_term_matrix, n_topics, max_iterations) \\n        # print topics\\n        topic_list = visualize_topics (path, topic_model, model_type, vectorizer.get_feature_names(), n_topics, n_top_words, save)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# EXECUTE\n",
    "# load data\n",
    "for path in paths:\n",
    "    data = load_data(path + doc_type[0])\n",
    "\n",
    "    if (model_type in model_types):\n",
    "        # choose vectorizer     min_df = 2 ignorálja a csak 1x elôforduló szavakat, max_df = 1.0 alap\n",
    "#        vectorizer = vectorizer('tfidf', min_df = 2, max_df = 1.0)\n",
    "        vectorizer = vectorizer('tfidf', min_df = 1, max_df = 1.0)\n",
    "        # generate input matrix\n",
    "        doc_term_matrix = generate_input_matrix(vectorizer, data)\n",
    "        # do black magic\n",
    "        topic_model = generate_nmf_topic_model(doc_term_matrix, model_type, n_topics, max_iterations)\n",
    "        # print topics\n",
    "        topic_list = visualize_topics (path, topic_model, model_type, vectorizer.get_feature_names(), n_topics, n_top_words, save)\n",
    "    \n",
    "    else:\n",
    "        # choose vectorizer    min_df = 2 ignorálja a csak 1x elôforduló szavakat, max_df = 1.0 alap\n",
    "#        vectorizer = vectorizer('tf', min_df = 2, max_df = 1.0)\n",
    "        vectorizer = vectorizer('tf', min_df = 1, max_df = 1.0)\n",
    "        # generate input matrix\n",
    "        doc_term_matrix = generate_input_matrix(vectorizer, data)\n",
    "        # do black magic\n",
    "        topic_model = generate_lda_topic_model(doc_term_matrix, n_topics, max_iterations) \n",
    "        # print topics\n",
    "        topic_list = visualize_topics (path, topic_model, model_type, vectorizer.get_feature_names(), n_topics, n_top_words, save)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
