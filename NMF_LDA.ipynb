{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANEV\\Anaconda3\\lib\\site-packages\\past\\types\\oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "C:\\Users\\ANEV\\Anaconda3\\lib\\site-packages\\past\\builtins\\misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n"
     ]
    }
   ],
   "source": [
    "# IMPORT PACKAGES\n",
    "import spacy, pandas, numpy, string\n",
    "from spacy.lang.de import German\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA S.T. 1 LINE IN XLSX = 1 DOCUMENT\n",
    "def load_data (path, one_doc_per_person):\n",
    "    data_raw = open(path + '.csv', encoding = 'utf-8').read().replace('\\\"', '').replace('\\ufeff', '')\n",
    "    data_list = data_raw.split('\\n')\n",
    "    \n",
    "    data_list_remove_empty_last_line = []\n",
    "    for row in range(0, len(data_list)-1):\n",
    "        data_list_remove_empty_last_line.append(data_list[row])\n",
    "    \n",
    "    # rebuild the \"raw\" data by combining records of the same person\n",
    "    # !!! metadata (date, temporal focus, interlocutor) are invalid\n",
    "    if one_doc_per_person:\n",
    "        rec_dict = {}\n",
    "        age_dict = {}\n",
    "        num_cols = data_list_remove_empty_last_line[0].count(';') + 1\n",
    "        for row in data_list_remove_empty_last_line:\n",
    "            cells = row.split(';')\n",
    "            words = cells[0]\n",
    "            id = cells[1]\n",
    "            age = cells[2]\n",
    "\n",
    "            if id in rec_dict:\n",
    "                rec_dict[id] = rec_dict[id] + ' ' + words\n",
    "            else:\n",
    "                rec_dict[id] = words\n",
    "\n",
    "            age_dict[id] = age\n",
    "        # now we have full dictionaries -> rewrite data_list_remove_empty_last_line\n",
    "        data_list_remove_empty_last_line = \\\n",
    "            [rec_dict[id] + ';' + id + ';' + age_dict[id] + ';2000_01_01' + (num_cols-4) * ';0' for id in rec_dict]\n",
    "\n",
    "        \n",
    "    input_table = [row.split(';') for row in data_list_remove_empty_last_line]      \n",
    "    \n",
    "#    print(path + '.csv: data loaded')\n",
    "    return data_list_remove_empty_last_line, input_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE TF / IDF / TF-IDF VALUES\n",
    "def vectorize (vectorizer_type, min_df, max_df):\n",
    "    \n",
    "    if vectorizer_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(max_df = max_df,\n",
    "                                     min_df = min_df)\n",
    "#        print('Tfidf-vectorizing done')\n",
    "        return vectorizer\n",
    "    \n",
    "    elif vectorizer_type == 'tf':\n",
    "        vectorizer = CountVectorizer(max_df = max_df,\n",
    "                                     min_df = min_df)\n",
    "#        print('Count-vectorizing done')\n",
    "        return vectorizer                 \n",
    "            \n",
    "    else:\n",
    "        print('error: unknown vectorizer')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE SPARSE DOC-TERM MATRIX \n",
    "def generate_input_matrix(vectorizer, input_table):\n",
    "    t = time()\n",
    "\n",
    "    column1 = [row[0] for row in input_table] \n",
    "    doc_term_matrix = vectorizer.fit_transform(column1)\n",
    "    \n",
    "#    print('doc-term matrix generated in %s s' %round((time() - t), 5) + ', matrix dimensions: ' + str(doc_term_matrix.shape))\n",
    "    return doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF, Source: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "def generate_nmf_topic_model(doc_term_matrix, beta_loss, n_topics, max_iterations):\n",
    "    t = time()\n",
    "\n",
    "    # NMF - Frobenius-norm : ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2\n",
    "    # math: d_{Fro}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2\n",
    "    if beta_loss == 'frobenius':\n",
    "        matrix_factorization = NMF(beta_loss = beta_loss, n_components = n_topics, max_iter = max_iterations)\n",
    "        nmf_model = matrix_factorization.fit(doc_term_matrix)\n",
    "#        print('NMF Frobenius topic_model created in %s s' %round((time() - t), 5))\n",
    "\n",
    "    # NMF - Kullback-Leibler divergence:\n",
    "    # math: d_{KL}(X, Y) = \\sum_{i,j} (X_{ij} log(\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})\n",
    "    elif beta_loss == 'kullback-leibler':\n",
    "        matrix_factorization = NMF(beta_loss = beta_loss, n_components = n_topics, max_iter = max_iterations, solver = 'mu')\n",
    "        nmf_model = matrix_factorization.fit(doc_term_matrix)\n",
    "#        print('NMF Kullback-Leibler topic_model created in %s s' %round((time() - t), 5))           \n",
    "    \n",
    "    else:\n",
    "        print('error: invalid beta_loss')\n",
    "        \n",
    "    return matrix_factorization, nmf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA, Source: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "def generate_lda_topic_model(doc_topic_prior, topic_word_prior, doc_term_matrix, n_topics, max_iterations, learning_method, learning_offset):\n",
    "#def generate_lda_topic_model(doc_term_matrix, n_topics, max_iterations, learning_method, learning_offset):    \n",
    "    t = time()\n",
    "    matrix_factorization = LatentDirichletAllocation(n_components = n_topics,\n",
    "                                                    doc_topic_prior = doc_topic_prior,\n",
    "                                                    topic_word_prior = topic_word_prior, \n",
    "                                                    learning_method = learning_method,\n",
    "                                                    learning_offset = learning_offset, \n",
    "                                                    max_iter = max_iterations)\n",
    "    \n",
    "    lda_model = matrix_factorization.fit(doc_term_matrix)\n",
    "#    print('LDA topic_model created in %s s' %round((time() - t), 5))     \n",
    "    \n",
    "    return matrix_factorization, lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT TOPICS, Source: https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "def print_topics (path, topic_model, feature_names, n_top_words, save):\n",
    "\n",
    "    # calculates the top words of topics                        \n",
    "    if (save):\n",
    "        with open(path + '_topics.csv', 'w', encoding = 'latin-1') as doc_out:\n",
    "            for idx, topic in enumerate(topic_model.components_):\n",
    "                topic_list = \"#%d, \" % (idx + 1)\n",
    "                topic_list += \" \".join([\"{} ({}),\".format(feature_names[idx], str(round(topic[idx], 5))) for idx in topic.argsort()[:-n_top_words - 1:-1]]) \n",
    "                doc_out.write(topic_list + '\\n')\n",
    "                \n",
    "    else:\n",
    "        for idx, topic in enumerate(topic_model.components_):\n",
    "            topic_list = \"Topic #%d: \" % (idx + 1)\n",
    "            topic_list += \" \".join([\"{} ({}),\".format(feature_names[idx], str(round(topic[idx], 5))) for idx in topic.argsort()[:-n_top_words - 1:-1]]) \n",
    "            print(topic_list)\n",
    "\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE AND PRINT TOPIC DISTRIBUTION\n",
    "def calculate_topic_distribution(path, topic_model, doc_term_matrix, data_list, feature_names): \n",
    "\n",
    "    # TRANSFORM DATA INTO DATAFRAME  \n",
    "    row_index = 1\n",
    "    rows = []\n",
    "    for row in data_list:\n",
    "        rows.append(str(row_index) + ';' + row)\n",
    "        row_index = row_index + 1\n",
    "    \n",
    "    columns = [\"Topic #%d: \" % (index + 1) for index, topic in enumerate(topic_model.components_)]\n",
    "    values = numpy.round(topic_model.transform(doc_term_matrix), 5)\n",
    "    df = pandas.DataFrame(values, rows, columns)\n",
    "    df.index.names = ['Row;Record;ID;Age;Date;W;Partner;Family;Friend;Stranger;Past;Future']\n",
    "\n",
    "    # CALCULATE DOMINANT TOPIC / DOC BUT MASK IT WHEN ALL TOPICS HAVE EQUAL WEIGHT\n",
    "    dominant_topic = numpy.argmax(df.values, axis = 1) + 1 \n",
    "    flat_distr = numpy.equal(numpy.amax(df.values, axis = 1), numpy.amin(df.values, axis = 1))\n",
    "    numpy.putmask(dominant_topic, flat_distr, 0)\n",
    "    df['dominant_topic'] =   dominant_topic\n",
    "    df = df.sort_values('dominant_topic')\n",
    "    \n",
    "    # PRINT TO CSV\n",
    "    df.to_csv(path + '_distr.csv', sep=';', columns=None, header=True, index=True, index_label=None, mode='w', encoding='utf-8', compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.')\n",
    "    \n",
    "#    print('topic distribution is done and printed')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF FILE GENERATOR\n",
    "def generate_nmf_models(max_dfs, min_dfs, n_top_words, max_iterations, data_list, input_table, save):\n",
    "    max_dfs = max_dfs\n",
    "    min_dfs = min_dfs\n",
    "    \n",
    "    for max_df in max_dfs:\n",
    "        for min_df in min_dfs:\n",
    "            for n_topics in range(8, 9):\n",
    "\n",
    "                model_specification_in_file_name = '_min' + str(min_df) + '_max' + str(max_df) + '_k' + str(n_topics)\n",
    "                print('in current iteration, min_df=' + str(min_df) + ', max_df=' + str(max_df) + ', number of topics=' + str(n_topics))\n",
    "    \n",
    "                tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "                tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "    \n",
    "                tfidf_vectorizer = vectorize ('tfidf', min_df, max_df)\n",
    "                tfidf_doc_term_matrix = generate_input_matrix (tfidf_vectorizer, input_table)\n",
    "\n",
    "            # PREPARE NMF TOPIC MODEL\n",
    "                path = './IO_YO/NMF_TFIDF_FR' + model_specification_in_file_name\n",
    "                matrix_factorization, topic_model = generate_nmf_topic_model (tfidf_doc_term_matrix, 'frobenius', n_topics, max_iterations)\n",
    "                topic_list = print_topics (path, topic_model, tfidf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "                df = calculate_topic_distribution(path, topic_model, tfidf_doc_term_matrix, data_list, tfidf_vectorizer.get_feature_names())\n",
    "                print('nmf with TFIDF and Frobenius done')\n",
    "\n",
    "            # PREPARE NMF TOPIC MODEL\n",
    "                path = './IO_YO/NMF_TFIDF_KL' + model_specification_in_file_name\n",
    "                matrix_factorization, topic_model = generate_nmf_topic_model (tfidf_doc_term_matrix, 'kullback-leibler', n_topics, max_iterations)\n",
    "                topic_list = print_topics (path, topic_model, tfidf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "                df = calculate_topic_distribution(path, topic_model, tfidf_doc_term_matrix, data_list, tfidf_vectorizer.get_feature_names())\n",
    "                print('nmf with TFIDF and Kullback-Leibler done')\n",
    "    \n",
    "            # PREPARE NMF TOPIC MODEL\n",
    "                path = './IO_YO/NMF_TF_FR' + model_specification_in_file_name\n",
    "                matrix_factorization, topic_model = generate_nmf_topic_model (tf_doc_term_matrix, 'frobenius', n_topics, max_iterations)\n",
    "                topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "                df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())    \n",
    "                print('nmf with TF and Frobenius done')\n",
    "                \n",
    "                if (max_df == 70 and min_df == 2 and n_topics == 8):\n",
    "                    pyLDAvis.enable_notebook()\n",
    "                    pyLDAvis.sklearn.prepare(matrix_factorization, tf_doc_term_matrix, tf_vectorizer)\n",
    "\n",
    "            # PREPARE NMF TOPIC MODEL\n",
    "                path = './IO_YO/NMF_TF_KL' + model_specification_in_file_name\n",
    "                matrix_factorization, topic_model = generate_nmf_topic_model (tf_doc_term_matrix, 'kullback-leibler', n_topics, max_iterations)\n",
    "                topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "                df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names())    \n",
    "                print('nmf with TF and Kullback-Leibler done')\n",
    "                    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA FILE GENERATOR\n",
    "def generate_lda_models(max_dfs, min_dfs, n_top_words, alphas, betas, max_iterations, data_list, input_table, save):\n",
    "    \n",
    "    alphas = alphas\n",
    "    betas = betas\n",
    "    \n",
    "    for max_df in max_dfs:\n",
    "        for min_df in min_dfs:\n",
    "            \n",
    "            tf_vectorizer = vectorize ('tf', min_df, max_df)\n",
    "            tf_doc_term_matrix = generate_input_matrix (tf_vectorizer, input_table) \n",
    "\n",
    "            for alpha in alphas:\n",
    "                for beta in betas:\n",
    "                    for n_topics in range(1, 41):\n",
    "                \n",
    "                        print('in current iteration, min_df=' + str(min_df) + ', max_df=' + str(max_df) + ', alpha=' + str(alpha) + ', beta=' + str(beta) + ', number of topics=' + str(n_topics))\n",
    "                        path = './IO_YO/lda' + '_min' + str(min_df) + '_max' + str(max_df) + '_alpha' + str(alpha) + '_beta' + str(beta) + '_k' + str(n_topics)\n",
    "            \n",
    "                        matrix_factorization, topic_model = generate_lda_topic_model (alpha, beta, tf_doc_term_matrix, n_topics, max_iterations, learning_method = 'online', learning_offset = 50.) #, random_state = 0\n",
    "                        topic_list = print_topics (path, topic_model, tf_vectorizer.get_feature_names(), n_top_words, save)\n",
    "                        df = calculate_topic_distribution(path, topic_model, tf_doc_term_matrix, data_list, tf_vectorizer.get_feature_names()) \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in current iteration, min_df=2, max_df=70, number of topics=8\n",
      "nmf with TFIDF and Frobenius done\n",
      "nmf with TFIDF and Kullback-Leibler done\n",
      "nmf with TF and Frobenius done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANEV\\Anaconda3\\lib\\site-packages\\pyLDAvis\\sklearn.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return dists / dists.sum(axis=1)[:, None]\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "\n * Not all rows (distributions) in doc_topic_dists sum to 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-be75093fdc35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./IO_YO/all_2Snorm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_doc_per_person\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mgenerate_nmf_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_dfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_dfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#generate_lda_models(max_dfs, min_dfs, n_top_words, alphas, betas, max_iterations, data_list, input_table, save)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-ccb98959c7d8>\u001b[0m in \u001b[0;36mgenerate_nmf_models\u001b[1;34m(max_dfs, min_dfs, n_top_words, max_iterations, data_list, input_table, save)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_df\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m70\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mn_topics\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                     \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix_factorization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_doc_term_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;31m# PREPARE NMF TOPIC MODEL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyLDAvis\\sklearn.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m     94\u001b[0m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[0;32m    372\u001b[0m    \u001b[0mdoc_lengths\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'doc_length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m    \u001b[0mvocab\u001b[0m            \u001b[1;33m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocab'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m    \u001b[0m_input_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m    \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     63\u001b[0m    \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m' * '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: \n * Not all rows (distributions) in doc_topic_dists sum to 1."
     ]
    }
   ],
   "source": [
    "# TUNERS\n",
    "#doc_types = ['1norm', '1Snorm', '2norm', '2Snorm']\n",
    "alphas = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "betas = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "max_dfs = [120, 90, 70]\n",
    "#max_dfs = [60, 51, 45]\n",
    "min_dfs = [1, 2]\n",
    "n_top_words = 5\n",
    "max_iterations = 200\n",
    "save = True    #True = prints distr and topics to file, False = only prints topics to console\n",
    "one_doc_per_person = False     #True: 1doc = 1person, False: 1doc = 1record\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# EXECUTE\n",
    "\n",
    "data_list, input_table = load_data('./IO_YO/all_2Snorm', one_doc_per_person)\n",
    "\n",
    "generate_nmf_models(max_dfs, min_dfs, n_top_words, max_iterations, data_list, input_table, save)\n",
    "\n",
    "#generate_lda_models(max_dfs, min_dfs, n_top_words, alphas, betas, max_iterations, data_list, input_table, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
