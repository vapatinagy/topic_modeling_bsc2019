{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "# Load German tokenizer, tagger, parser, NER and word vectors, source: https://spacy.io/models/de\n",
    "import spacy, string\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "from spacy.lang.de import German\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# regular expressions, source: https://docs.python.org/3.5/library/re.html\n",
    "import re, os\n",
    "\n",
    "# EACH FUNCTION IS RESPONSIBLE FOR A SINGLE STEP IN THE NORMALIZATION PROCESS.\n",
    "# EACH FUNCTION HAS A SAVE OPTION THAT ALWAYS OVERWRITES THE SAME FILE AND SERVES PURELY AS A CHECK-POINT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATASET\n",
    "def load_data (path): \n",
    "    try:\n",
    "        data_raw = open(path + '_in.csv', encoding = 'latin-1').read().replace('\\\"', '')\n",
    "    except FileNotFoundError:\n",
    "        print('file ' + path + '_in.csv not found')\n",
    "        \n",
    "    print('data load is done')\n",
    "    return data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION AND POS-TAGGING\n",
    "def tokenize (data, save):\n",
    "    data_doc = nlp(data)    \n",
    "\n",
    "    if (save):\n",
    "        save_checkpoint(data_doc, 'tok')\n",
    "\n",
    "    print('tokenization and pos-tagging are done')\n",
    "    return data_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE PUNCTUATION\n",
    "def remove_punctuation (data, save):\n",
    "    data_nopunct = [token for token in data if (token.is_punct == False and token.text.lower() != 'so.')] \n",
    "    \n",
    "    if (save):\n",
    "        save_checkpoint(data_nopunct, 'pun') \n",
    " \n",
    "    print('punctuation removal is done')\n",
    "    return data_nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE FOLDING - MAPPING ALL WORDS TO LOWER CASE\n",
    "def lowerize (data, save):\n",
    "    data_lower = [token.text.lower() for token in data]\n",
    "    \n",
    "    if (save):\n",
    "        save_checkpoint(data_lower, 'low') \n",
    "\n",
    "    print('case folding is done')\n",
    "    return data_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE OPERATOR LABELS, NUMBERS AND OTHER NOISE FROM TEXT\n",
    "def reduce_noise (data, save):\n",
    "    complex_white_space = re.compile('^\\s*\\n+\\s*$')\n",
    "    \n",
    "    # removes labels added by operators\n",
    "    noise = {'xxxx', 'xxx', 'xx', 'yyyy', 'yyy', 'zzz', '*'}\n",
    "    data_noxy = [word.replace('xxx-', '').replace('xxx', '').replace('.yyy', '').replace('yyy', '').replace('*', '') for word in data if word.strip() not in noise]\n",
    "\n",
    "    #removes 'ß', ''s'\n",
    "    data_ss = [word.replace('ß', 'ss').replace('\\'s', '').replace('’s', '') for word in data_noxy]\n",
    "    \n",
    "    # deletes numbers written as numbers\n",
    "    data_num = [word for word in data_ss if not word.isdigit()]\n",
    "\n",
    "    # ignore words of a single character\n",
    "    data_nsc = [word for word in data_num if (len(word) >= 2 or complex_white_space.match(word))]\n",
    "    \n",
    "    data_clean = data_nsc\n",
    "\n",
    "    if (save):\n",
    "        save_checkpoint(data_clean, 'noi') \n",
    "\n",
    "    print('noise reduction is done')\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEMMATIZATION\n",
    "def lemmatize (data, save): \n",
    "    data_t = string_to_token(data)\n",
    "    data_lemmatized = [token.lemma_.replace('ß', 'ss') for token in data_t]\n",
    "    \n",
    "    if (save):\n",
    "        save_checkpoint(data_lemmatized, 'lem') \n",
    "\n",
    "    print('lemmatization is done')\n",
    "    return data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTEND STOPWORD LIST, REMOVE STOPWORDS\n",
    "def remove_stopwords (data, save):\n",
    "    MY_STOP_WORDS = nlp(open('MY_STOP_WORDS.txt').read())\n",
    "    for token in MY_STOP_WORDS:\n",
    "        if token not in STOP_WORDS:\n",
    "            STOP_WORDS.add(token.text)\n",
    "        \n",
    "    data_nostops = [word for word in data if word not in STOP_WORDS]\n",
    "    \n",
    "    if (save):\n",
    "        save_checkpoint(data_nostops, 'sto')  \n",
    "\n",
    "    print('stop word removal is done')\n",
    "    return data_nostops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTEND STOPWORD LIST, REMOVE STOPWORDS\n",
    "def remove_stopwords2 (data, save):\n",
    "    MY_STOP_WORDS = nlp(open('MY_STOP_WORDS.txt').read())\n",
    "    ja = ['jajajajaja','jajajaja','jajaja','jaja','jaaa','jaa','jja','naja','tja']\n",
    "    nein = ['neinnein','neein','neine','nein','nei']\n",
    "    laugh = ['hahaha','haha','hihihi','hihi','hehehe','hehe','hohoho','hoho']\n",
    "    hurrah = ['yay', 'yeah', 'ey']\n",
    "    bla = ['blablablablabla','blablablabla','blablabla']\n",
    "    hm = ['hm', 'hmm', 'mmh', 'mmmm', 'mmm', 'mm']\n",
    "    mhm = ['mhh','mh']\n",
    "    ah = ['aaah','aah','ahh','ach','ooh','ohh','oh','uh']\n",
    "    öhm = ['eh', 'ähm', 'äh', 'ööh', 'öh']\n",
    "    hä = ['ha', 'he', 'hää', 'häh', 'hö']\n",
    "\n",
    "    for word in data:\n",
    "        word.replace('naja', 'na ja'\n",
    "                ).replace('ahja', 'ah ja'\n",
    "                         ).replace('ahso', 'ah so'\n",
    "                                  ).replace('okay', 'ok'\n",
    "                                           ).replace('ahaa', 'aha'\n",
    "                                                    ).replace('whoa', 'wow'\n",
    "                                                             ).replace('nöö', 'nö'\n",
    "                                                                      ).replace('jöö', 'jö'\n",
    "                                                                               ).replace('hey', 'hei'\n",
    "                                                                                        ).replace('oups', 'ups'\n",
    "                                                                                                ).replace('eieiei', 'eiei')\n",
    "\n",
    "        if word in ja:\n",
    "            word == 'ja'\n",
    "        elif word in nein:\n",
    "            word == 'nein'\n",
    "        elif word in laugh:\n",
    "            word == 'haha'\n",
    "        elif word in hurrah:\n",
    "            word == 'yey'\n",
    "        elif word in bla:\n",
    "            word == 'blabla'\n",
    "        elif word in hm:\n",
    "            word == 'hm'\n",
    "        elif word in mhm:\n",
    "            word == 'mhm'\n",
    "        elif word in ah:\n",
    "            word == 'ah'\n",
    "        elif word in öhm:\n",
    "            word == 'öhm'\n",
    "        elif word in hä:\n",
    "            word == 'hä'\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    data_nostops = data\n",
    "    \n",
    "    if (save):\n",
    "        save_checkpoint(data_nostops, 'sto')  \n",
    "   \n",
    "    print('stop word removal is done')\n",
    "    return data_nostops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECONVERT STRINGS TO TOKENS - HELPER METHOD\n",
    "def string_to_token (data):\n",
    "    data_s = ''\n",
    "    for word in data:\n",
    "        data_s = data_s + word + ' '\n",
    "    data_t = nlp(data_s)\n",
    "\n",
    "    return data_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE CHECKPOINT - HELPER METHOD\n",
    "def save_checkpoint(data, step):\n",
    "    with open(path + '_' + step + '.csv', 'w', encoding = 'latin-1') as doc:\n",
    "        \n",
    "        if isinstance(data[0], str):\n",
    "            for element in data:\n",
    "                doc.write(element + ' ') \n",
    "        else:  \n",
    "            for element in data:\n",
    "                doc.write(element.text + ' ') \n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE OUTPUT INTO FILE\n",
    "def save_output (data, path, print_type):\n",
    "    white_space = re.compile('^\\s+$')\n",
    "    complex_white_space = re.compile('^\\s*\\n+\\s*$')\n",
    "    \n",
    "    with open(path + '_norm.csv', 'w', encoding = 'latin-1') as doc_out:\n",
    "\n",
    "        # save doc as running text\n",
    "        if print_type == 'flow':\n",
    "            for word in data: \n",
    "                if white_space.match(word):\n",
    "                    continue\n",
    "                else:\n",
    "                    doc_out.write(word + ' ') \n",
    "            print('output is saved')\n",
    "\n",
    "        # save doc as a list of words    \n",
    "        elif print_type == 'list':\n",
    "            for word in data: \n",
    "                if white_space.match(word):\n",
    "                    continue\n",
    "                doc_out.write(word.strip() + '\\n') \n",
    "            print('output is saved')\n",
    "\n",
    "        # check if 1st item is white space\n",
    "        elif print_type == 'record':\n",
    "            if complex_white_space.match(data[0]) == None:\n",
    "                doc_out.write(data[0].strip() + ' ')\n",
    "\n",
    "            # save doc s.t. skip white space\n",
    "            for i in range(1, len(data)-2):\n",
    "                if (complex_white_space.match(data[i]) and complex_white_space.match(data[i+1])):\n",
    "                    continue\n",
    "                if complex_white_space.match(data[i]):\n",
    "                    doc_out.write('\\n')\n",
    "                else:\n",
    "                    doc_out.write(data[i].strip() + ' ')  \n",
    "                \n",
    "            # check if last item is white space        \n",
    "            if complex_white_space.match(data[len(data)-1]) == None:\n",
    "                doc_out.write(data[len(data)-1].strip())   \n",
    "            print('output is saved')\n",
    "                \n",
    "        else:\n",
    "            print('error: invalid print_type in save_output()')\n",
    "\n",
    "    return doc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# TUNERS\\nsave = True\\nprint_type = 'record'\\n\\n# tuners for group level analysis\\npaths = ['./IO_YO/young', './IO_YO/old']\\n\\n# tuners for person level analysis\\n#participant_ids = nlp(open('PARTICIPANT_ID.txt').read())\\n#paths = ['./IO_P/' + token.text for token in participant_ids]\\n\\n# EXECUTE\\nfor path in paths:\\n    if path is not './IO_P/\\n':\\n        print(path)\\n        save_output(remove_stopwords\\n                    (lemmatize\\n                     (reduce_noise\\n                      (lowerize\\n                       (remove_punctuation\\n                        (tokenize\\n                         (load_data(\\n                            path),save), \\n                         save), \\n                        save), \\n                       save), \\n                      save), \\n                     save), path, print_type)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# TUNERS\n",
    "save = True\n",
    "print_type = 'record'\n",
    "\n",
    "# tuners for group level analysis\n",
    "paths = ['./IO_YO/young', './IO_YO/old']\n",
    "\n",
    "# tuners for person level analysis\n",
    "#participant_ids = nlp(open('PARTICIPANT_ID.txt').read())\n",
    "#paths = ['./IO_P/' + token.text for token in participant_ids]\n",
    "\n",
    "# EXECUTE\n",
    "for path in paths:\n",
    "    if path is not './IO_P/\\n':\n",
    "        print(path)\n",
    "        save_output(remove_stopwords\n",
    "                    (lemmatize\n",
    "                     (reduce_noise\n",
    "                      (lowerize\n",
    "                       (remove_punctuation\n",
    "                        (tokenize\n",
    "                         (load_data(\n",
    "                            path),save), \n",
    "                         save), \n",
    "                        save), \n",
    "                       save), \n",
    "                      save), \n",
    "                     save), path, print_type)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load is done\n",
      "tokenization and pos-tagging are done\n",
      "punctuation removal is done\n",
      "case folding is done\n",
      "noise reduction is done\n",
      "lemmatization is done\n",
      "stop word removal is done\n",
      "output is saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# EXECUTE - VERSION FOR TESTS\n",
    "save = True\n",
    "print_type = 'record'\n",
    "paths = ['./IO_YO/young', './IO_YO/old', './IO_P/p1-y-ph1', './IO_P/p100-y-ph8']\n",
    "path =  paths[1]\n",
    "\n",
    "a = load_data(path)\n",
    "b = tokenize(a, save)  \n",
    "c = remove_punctuation(b, save)\t\n",
    "d = lowerize (c, save)\t\n",
    "e = reduce_noise(d, save)\n",
    "f = lemmatize (e, save)\n",
    "g = remove_stopwords (f, save)\n",
    "h = save_output (g, path, print_type)\n",
    "#print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
