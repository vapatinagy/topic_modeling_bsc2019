{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "import spacy, string\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "from spacy.lang.de import German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA S.T. 1 LINE IN XLSX = 1 DOCUMENT\n",
    "def load (path):\n",
    "    data_raw = open(path + '.csv', encoding = 'utf-8').read().replace('\\\"', '').replace('\\ufeff', '')\n",
    "    data_1row_1string = data_raw.split('\\n')\n",
    "    data_1row_stringlist = [row.split(';') for row in data_1row_1string] #ebbe kell a végén visszatölteni, a.k.a. row[0]-t felülírni\n",
    "    \n",
    "    print('data from ' + path + ' is loaded')\n",
    "    return data_1row_stringlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT TRANSCRIPTIONS\n",
    "def extract (data_1row_stringlist):\n",
    "    column1 = [row[0] for row in data_1row_stringlist]\n",
    "    column1_1row_tokenlist = [nlp(row) for row in column1]\n",
    "    print('column extraction is done')\n",
    "    \n",
    "    return column1_1row_tokenlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization, spacy lemmatization\n",
    "def normalize (column1_1row_tokenlist):\n",
    "    column1_normalized = []\n",
    "    for row in column1_1row_tokenlist:\n",
    "        row_lemmatized = [token.lemma_.lower().replace('ß', 'ss').replace('\\'s', '').replace('’s', '') for token in row if (not token.text.isdigit() and token.is_punct == False and len(token) > 1)]\n",
    "        column1_normalized.append(row_lemmatized)\n",
    "    print('spacy normalization is done')\n",
    "    \n",
    "    return column1_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization with gerTwol\n",
    "def gertwol_optimize (column1_normalized):\n",
    "\n",
    "    gertwol_raw = nlp(open('GERTWOL_LIST.csv', encoding = 'utf-8').read().replace('\\ufeff', ''))\n",
    "    gertwol_list = [row.text.split(';') for row in gertwol_raw if row.text != '\\n'] \n",
    "\n",
    "    lemma_dict = {}\n",
    "    for i in range(0, len(gertwol_list)-1):    \n",
    "        row = gertwol_list[i]\n",
    "        lemma_dict[row[0]] = [row[1]]\n",
    "\n",
    "    column1_gertwoled = []\n",
    "    j = 0\n",
    "    with open('Test.csv', 'w') as test:\n",
    "        for row in column1_normalized:\n",
    "            for i in range (0, len(row)-1):\n",
    "                if (row[i] in lemma_dict.keys() and row[i] != lemma_dict[row[i]][0]):\n",
    "                    test.write(row[i] + ': ' + lemma_dict[row[i]][0] + '\\n')\n",
    "                    row[i] = lemma_dict[row[i]][0] #returns value\n",
    "                    j = j + 1\n",
    "            column1_gertwoled.append(row)       \n",
    "\n",
    "    print('gertwol optimization is done, nr of changed items: ' + str(j))\n",
    "    return column1_gertwoled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE STOPWORDS\n",
    "def remove_stopwords (column1):\n",
    "    column1_nostops = []\n",
    "    stopwords = open('STOPWORDS.csv', encoding = 'utf-8').read().replace('\\ufeff', '').split('\\n')\n",
    "\n",
    "    for row in column1:      \n",
    "        row_nostops = [word for word in row if word not in stopwords]\n",
    "        column1_nostops.append(row_nostops) \n",
    "       \n",
    "    print('stopword removal is done')\n",
    "    return column1_nostops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert transcription back to table and print\n",
    "def reinsert_and_save(data_1row_stringlist, column1_normalized, path):\n",
    "    i = 0\n",
    "    for row in column1_normalized:\n",
    "        data_to_string = ''\n",
    "        for word in row:\n",
    "            data_to_string = data_to_string + word + ' ' \n",
    "        data_1row_stringlist[i][0] = data_to_string\n",
    "        i = i + 1\n",
    "   \n",
    "    with open(path + 'norm.csv', 'w', encoding = 'utf-8') as doc_out:\n",
    "        for row in data_1row_stringlist:\n",
    "            for cell in row:\n",
    "                doc_out.write(cell + ';')\n",
    "            doc_out.write('\\n')\n",
    "    print(path + ' output is saved')\n",
    "\n",
    "    return doc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data from ./IO_YO/all_in_v2 is loaded\n",
      "column extraction is done\n",
      "spacy normalization is done\n",
      "gertwol optimization is done, nr of changed items: 8091\n",
      "./IO_YO/all_1 output is saved\n",
      "./IO_YO/all_2 output is saved\n",
      "stopword removal is done\n",
      "./IO_YO/all_1S output is saved\n",
      "stopword removal is done\n",
      "./IO_YO/all_2S output is saved\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE\n",
    "paths = ['./IO_YO/all', './IO_YO/Test']\n",
    "\n",
    "doc_in = load(paths[0] + '_in_v2')\n",
    "doc_norm = normalize(extract(doc_in))\n",
    "doc_gert = gertwol_optimize (doc_norm) \n",
    "\n",
    "doc_out = reinsert_and_save(doc_in, doc_norm, paths[0] + '_1')\n",
    "doc_out = reinsert_and_save(doc_in, doc_gert, paths[0] + '_2')    \n",
    "doc_out = reinsert_and_save(doc_in, remove_stopwords (doc_norm), paths[0] + '_1S')    \n",
    "doc_out = reinsert_and_save(doc_in, remove_stopwords (doc_gert), paths[0] + '_2S') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
