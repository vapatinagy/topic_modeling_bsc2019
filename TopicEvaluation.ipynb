{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, pandas, numpy, string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os.path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "def load_data(path):\n",
    "    \n",
    "    data_raw = open(path + '_distr.csv', encoding = 'utf-8').read()\n",
    "    data_list = data_raw.split('\\n') # this leaves an empty last line in the table\n",
    "    input_table = [row.split(';') for row in data_list]\n",
    "    \n",
    "#    print('data load from ' + path + ' is done')\n",
    "    return input_table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT DOMINANT TOPIC AND RECORDS FROM DATASET\n",
    "def extract_data(input_table):\n",
    "    \n",
    "    work_table = []\n",
    "    for i in range(1, len(input_table)-1):   \n",
    "        topic_vector = []\n",
    "        for j in range(12, len(input_table[i])-1):\n",
    "            if input_table[i][j] != '\"':\n",
    "                topic_vector.append(float(input_table[i][j].replace('\"', '')))\n",
    "            # FORMAT EXAMPLE: ['ROW_ID', 'RECORD', 'DOMINANT TOPIC', 'TOPIC VECTOR']\n",
    "        work_table.append([input_table[i][0], input_table[i][1], input_table[i][len(input_table[i])-1], topic_vector])   \n",
    "\n",
    "#    print('data extraction done')\n",
    "    return work_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUP TRANSCRIPTIONS BY DOMINANT TOPIC\n",
    "# CLUSTER FORMAT, EXAMPLE: ['DOMINANT TOPIC', 'TOPIC VECTOR', 'RECORD']\n",
    "# CLUSTER FORMAT, EXAMPLE: ['1', '0., 0.27, 0.41', 'mhm ja zürich']\n",
    "def create_clusters(work_table):  \n",
    "    \n",
    "    # CREATE AGGREGATION LEVELS\n",
    "    topics = []\n",
    "    for i in range(0, len(work_table)-1):\n",
    "        if work_table[i][2] not in topics:\n",
    "            topics.append(work_table[i][2])\n",
    "\n",
    "    clusters = [[dominant_topic, [], []] for dominant_topic in topics]\n",
    "   \n",
    "    # SPLIT INTO CLUSTERS\n",
    "    for i in range(0, len(work_table)-1):\n",
    "        for j in range (0, len(clusters)):\n",
    "            if (clusters[j][0] == work_table[i][2]):\n",
    "                clusters[j][1].append(numpy.asarray(work_table[i][3]))\n",
    "                clusters[j][2].append(work_table[i][1])\n",
    "\n",
    "#    print('clusters done')\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT TERMS AND WEIGHTS PER TOPIC\n",
    "# TERMS_ONLY EXAMPLE: [['danken', 'fallen', 'dienstag', 'frank', 'bitten'], ['warten', 'dürfen', 'abend', 'woche', 'tagen'], ['kennen', 'spielen', 'lernen', 'easy', 'echt']]\n",
    "# TERMS_WITH_WEIGHT EXAMPLE: [{'danken': '86.285', 'fallen': '78.262', 'dienstag': '68.248', 'frank': '62.249', 'bitten': '57.422'}, {'warten': '85.251', ...]\n",
    "def extract_topics(path_topics):\n",
    "\n",
    "    # LOAD\n",
    "    topics_raw = open(path_topics + '_topics.csv', encoding = 'latin-1').read()\n",
    "    topics_list = topics_raw.split('\\n')\n",
    "    topics_table = [topic.replace('(', ',').replace(')', ',').replace(' ', '').split(',') for topic in topics_list]\n",
    "    \n",
    "    topics_cleaned = []\n",
    "    for topic in topics_table:\n",
    "        cleaned = []\n",
    "        for i in range(1, len(topic)):\n",
    "            if (topic[i] != ''):\n",
    "                cleaned.append(topic[i])\n",
    "        topics_cleaned.append(cleaned)\n",
    "\n",
    "    # EXTRACT\n",
    "    terms_only = []\n",
    "    terms_with_weight = []\n",
    "    for i in range(0, len(topics_cleaned)-1):\n",
    "        term_weight_dict = {}\n",
    "        terms = []\n",
    "        for j in range (0, len(topics_cleaned[i])-1):\n",
    "            if j % 2 == 0:\n",
    "                terms.append(topics_cleaned[i][j])\n",
    "                term_weight_dict[topics_cleaned[i][j]] = float(topics_cleaned[i][j + 1])\n",
    "        terms_only.append(terms)\n",
    "        terms_with_weight.append(term_weight_dict)\n",
    "        \n",
    "#    print('topic extraction done')\n",
    "    return terms_only, terms_with_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATES VOCABULARY OF RECORDS\n",
    "def read_vocab():\n",
    "    data = open('./IO_YO/all_2Snorm.csv', encoding = 'utf-8').read()\n",
    "    data_list = data.split('\\n')\n",
    "    \n",
    "    vocab = set()\n",
    "    for row in data_list:\n",
    "        words = row[0 : row.find(';')]\n",
    "        [vocab.add(word) for word in words.split(' ') if len(word) > 0]\n",
    "    \n",
    "    return list(vocab) # use list to have a fixed order of elements (order not important, just that it is fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELBOW METHOD USING COSINE SIMILARITY OF TOPIC DISTRIBUTION VECTOR\n",
    "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html \n",
    "def calculate_cosine_distr (clusters):  \n",
    "    \n",
    "    # MEASURE COSINE SIMILARITY BASED ON TOPIC DISTRIBUTION VECTORS\n",
    "    avg = []\n",
    "    for c in range(0, len(clusters)):\n",
    "        vectors = numpy.asarray(clusters[c][1])\n",
    "        cs = cosine_similarity(vectors)\n",
    "        avg.append(numpy.average(cs))\n",
    "        \n",
    "    # CALCULATE MODEL AVERAGE\n",
    "    average = numpy.average(numpy.asarray(avg))\n",
    "    \n",
    "#    print('cosine similarity calculated')\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELBOW METHOD USING COSINE SIMILARITY OF DOCUMENT VECTOR\n",
    "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html \n",
    "def calculate_cosine_doc (clusters, vocabulary):  \n",
    "    \n",
    "    # MEASURE COSINE SIMILARITY BASED ON DOCUMENT VECTORS\n",
    "    avg = []\n",
    "    for cluster in clusters:\n",
    "        if (cluster[0] != 0): # ignore docs without dominant topic\n",
    "            vectors = []\n",
    "            for word_string in cluster[2]:\n",
    "                word_list = [w for w in word_string.split(' ') if len(w) > 0]\n",
    "                # create a vector of word counts, 1 entry for each word in the vocabulary\n",
    "                # equals to TF when used in calc. cosine similarity\n",
    "                word_vector = []\n",
    "                for w in vocabulary:\n",
    "                    word_vector.append(word_list.count(w))\n",
    "                vectors.append(word_vector)\n",
    "\n",
    "#            vectors = numpy.asarray(clusters[c][1])\n",
    "            cs = cosine_similarity(vectors)\n",
    "            avg.append(numpy.average(cs))\n",
    "        \n",
    "    # CALCULATE MODEL AVERAGE\n",
    "    if len(avg) > 0:\n",
    "        average = numpy.average(numpy.asarray(avg))\n",
    "    else:\n",
    "        average = 0\n",
    "    \n",
    "#    print('cosine similarity calculated')\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE SEMANTIC COHERENCE\n",
    "# Source: Mimno et al. 2011\n",
    "def calculate_coherence(work_table, work_topics):\n",
    "\n",
    "    #COUNT HOW MANY TIMES EACH WORD PAIR / TOPIC OCCUR TOGETHER IN DOCUMENTS\n",
    "    avg = []\n",
    "    for topic in range(0, len(work_topics)): \n",
    "        coherence = 0\n",
    "        for m in range(0, len(work_topics[0])-1):   \n",
    "            co_doc_frequency = 0     \n",
    "            for l in range (m + 1, len(work_topics[0])):\n",
    "                term2_frequency = 0.01\n",
    "                for row in range(0, len(work_table)):\n",
    "                    if (work_topics[topic][l] in work_table[row][1]):\n",
    "                        term2_frequency += 1\n",
    "                    if (work_topics[topic][m] in work_table[row][1] and work_topics[topic][l] in work_table[row][1]):\n",
    "                        co_doc_frequency += 1\n",
    "            coherence += numpy.log((co_doc_frequency + 1) / term2_frequency)\n",
    "        avg.append(coherence) \n",
    "\n",
    "    # CALCULATE MODEL AVERAGE\n",
    "    average = numpy.average(numpy.asarray(avg))\n",
    "\n",
    "#    print('coherence calculated')  \n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE EXCLUSIVITY\n",
    "def calculate_exclusivity(terms_with_weight):\n",
    "\n",
    "    # CALCULATE TERM WEIGHT IN A TOPIC COMPARED TO TERM WEIGHT IN ALL TOPICS\n",
    "    avg = []\n",
    "    for topic in terms_with_weight:\n",
    "        exclusivity = 0.00\n",
    "        for term, weight in topic.items():\n",
    "            term_weight_in_all_topics = 0.00\n",
    "            for topic_for_calc in terms_with_weight:\n",
    "                if topic_for_calc.get(term) != None:\n",
    "                    term_weight_in_all_topics += topic_for_calc[term]\n",
    "            exclusivity += weight / term_weight_in_all_topics \n",
    "        avg.append(exclusivity / len(topic.items()))\n",
    "    \n",
    "    # CALCULATE MODEL AVERAGE\n",
    "    average = numpy.average(numpy.asarray(avg))     \n",
    "\n",
    "#    print('exclusivity calculated')  \n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE, 1 PLOT DISPLAYS 4 MODELS ON TOP OF EACH OTHER\n",
    "def evaluate_nmf(max_dfs, min_dfs, models, n_topic_min, n_topic_max, n_topic_step, folder, cosine_basis):\n",
    "    \n",
    "    if cosine_basis == True:\n",
    "        # READ VOCABULARY\n",
    "        vocabulary = read_vocab()\n",
    "        \n",
    "    # INITIATE GRID PLOT (3X6)\n",
    "    num_cols = 3 # number of plot columns, equals to evaluation criteria (e.g. exclusivity) for topic models\n",
    "    num_rows = len(max_dfs)*len(min_dfs) # number of plot rows\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(num_cols*6.4, num_rows*4.8)) # add spacing and make it large (size in inches)\n",
    "    spec = fig.add_gridspec(ncols=num_cols, nrows=num_rows)\n",
    "    current_plot_row = 0\n",
    "    \n",
    "    n_topics_range = range(n_topic_min, n_topic_max + 1, n_topic_step)\n",
    "    cos_x_range = list(n_topics_range)\n",
    "    if cos_x_range[0] < 2:\n",
    "        del cos_x_range[0]\n",
    "\n",
    "    for max_df in max_dfs:\n",
    "        for min_df in min_dfs:\n",
    "        \n",
    "            big_cosine_list = []\n",
    "            big_coherence_list = []\n",
    "            big_exclusivity_list = []\n",
    "            big_cosine_labels = []     \n",
    "            big_coherence_labels = []   \n",
    "            big_exclusivity_labels = [] \n",
    "\n",
    "            for model in models:\n",
    "        \n",
    "                elbow_cosine_list = []     \n",
    "                elbow_coherence_list = []   \n",
    "                elbow_exclusivity_list = [] \n",
    "\n",
    "                for n_topics in n_topics_range:\n",
    "            \n",
    "                    print('in current iteration, model=' + model + ', min_df=' + str(min_df) + ', max_df=' + str(max_df) + ', number of topics=' + str(n_topics))\n",
    "                    path = folder + model + '_min' + str(min_df) + '_max' + str(max_df) + '_k' + str(n_topics)\n",
    "\n",
    "                    work_table = extract_data(load_data(path))\n",
    "                    terms_only, terms_with_weight = extract_topics(path) \n",
    "                    clusters = create_clusters(work_table)\n",
    "                \n",
    "                    # CALCULATE COSINE SIMILARITY\n",
    "                    if n_topics >= 2 :\n",
    "                        if cosine_basis == True:\n",
    "                            average_cosine = calculate_cosine_doc(clusters, vocabulary)  \n",
    "                        else:\n",
    "                            average_cosine = calculate_cosine_distr(clusters)   \n",
    "                        elbow_cosine_list.append(average_cosine)\n",
    "\n",
    "                    # CALCULATE COHERENCE  \n",
    "                    average_coherence = calculate_coherence(work_table, terms_only)\n",
    "                    elbow_coherence_list.append(average_coherence)\n",
    "    \n",
    "                    # CALCULATE EXCLUSIVITY\n",
    "                    average_exclusivity = calculate_exclusivity(terms_with_weight)\n",
    "                    elbow_exclusivity_list.append(average_exclusivity)    \n",
    "\n",
    "                big_cosine_list.append(elbow_cosine_list)\n",
    "                big_coherence_list.append(elbow_coherence_list)\n",
    "                big_exclusivity_list.append(elbow_exclusivity_list) \n",
    "                big_cosine_labels.append(model)\n",
    "                big_coherence_labels.append(model)\n",
    "                big_exclusivity_labels.append(model)  \n",
    "\n",
    "            # VISUALIZE COSINE SIMILARITY  \n",
    "            subplt = fig.add_subplot(spec[current_plot_row, 0])\n",
    "            for i in range(0, len(big_cosine_list)):\n",
    "                subplt.plot(\n",
    "                    cos_x_range,\n",
    "                    big_cosine_list[i],\n",
    "                    label = big_cosine_labels[i])\n",
    "            subplt.set_title('cosine sim. (min_df=' + str(min_df) + ', max_df=' + str(max_df) + ')')\n",
    "            subplt.legend(loc='best')\n",
    "        \n",
    "            # VISUALIZE COHERENCE\n",
    "            subplt = fig.add_subplot(spec[current_plot_row, 1])\n",
    "            for j in range(0, len(big_coherence_list)):\n",
    "                subplt.plot(\n",
    "                    n_topics_range,\n",
    "                    big_coherence_list[j],\n",
    "                    label = big_coherence_labels[j])\n",
    "            subplt.set_title('coherence (min_df=' + str(min_df) + ', max_df=' + str(max_df) + ')')\n",
    "            subplt.legend(loc='best')\n",
    "\n",
    "            # VISUALIZE EXCLUSIVITY\n",
    "            subplt = fig.add_subplot(spec[current_plot_row, 2])\n",
    "            for k in range(0, len(big_exclusivity_list)):\n",
    "                subplt.plot(\n",
    "                    n_topics_range,\n",
    "                    big_exclusivity_list[k],\n",
    "                    label = big_exclusivity_labels[k])\n",
    "            subplt.set_title('exclusivity (min_df=' + str(min_df) + ', max_df=' + str(max_df) + ')')\n",
    "            subplt.legend(loc='best')\n",
    "\n",
    "            current_plot_row = current_plot_row + 1         \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE, 1 PLOT DISPLAYS 4 MODELS ON TOP OF EACH OTHER\n",
    "def evaluate_lda(max_dfs, min_dfs, alphas, betas, n_topic_min, n_topic_max, n_topic_step, folder, cosine_basis):\n",
    "    \n",
    "    if cosine_basis == True:\n",
    "        # READ VOCABULARY\n",
    "        vocabulary = read_vocab()\n",
    "\n",
    "    # INITIATE GRID PLOT (3X6)\n",
    "    num_cols = 3 # number of plot columns, equals to evaluation criteria (e.g. exclusivity) for topic models\n",
    "    num_rows = len(max_dfs)*len(min_dfs) # number of plot rows\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(num_cols*6.4, num_rows*4.8)) # add spacing and make it large (size in inches)\n",
    "    spec = fig.add_gridspec(ncols=num_cols, nrows=num_rows)\n",
    "    current_plot_row = 0\n",
    "    \n",
    "    n_topics_range = range(n_topic_min, n_topic_max + 1, n_topic_step)         \n",
    "    cos_x_range = list(n_topics_range)\n",
    "    if cos_x_range[0] < 2:\n",
    "        del cos_x_range[0]\n",
    "    \n",
    "    for max_df in max_dfs:\n",
    "        for min_df in min_dfs:\n",
    "        \n",
    "            big_cosine_list = []\n",
    "            big_coherence_list = []\n",
    "            big_exclusivity_list = []\n",
    "            big_cosine_labels = []     \n",
    "            big_coherence_labels = []   \n",
    "            big_exclusivity_labels = [] \n",
    "\n",
    "            for alpha in alphas:\n",
    "                for beta in betas:\n",
    "                \n",
    "                    model = 'a=' + str(alpha) + '/b=' + str(beta)\n",
    "    \n",
    "                    elbow_cosine_list = []     \n",
    "                    elbow_coherence_list = []   \n",
    "                    elbow_exclusivity_list = [] \n",
    "\n",
    "                    for n_topics in n_topics_range:\n",
    "\n",
    "                        print('in current iteration, min_df=' + str(min_df) + ', max_df=' + str(max_df) + ', alpha=' + str(alpha) + ', beta=' + str(beta) + ', number of topics=' + str(n_topics))\n",
    "                        path = folder + 'lda' + '_min' + str(min_df) + '_max' + str(max_df) + '_alpha' + str(alpha) + '_beta' + str(beta) + '_k' + str(n_topics)   \n",
    "                    \n",
    "                        work_table = extract_data(load_data(path))\n",
    "                        terms_only, terms_with_weight = extract_topics(path) \n",
    "                        clusters = create_clusters(work_table)\n",
    "                        \n",
    "                        average_cosine = None\n",
    "                        # CALCULATE COSINE SIMILARITY\n",
    "                        if n_topics >= 2 :\n",
    "                            if cosine_basis == True:\n",
    "                                average_cosine = calculate_cosine_doc(clusters, vocabulary)  \n",
    "                            else:\n",
    "                                average_cosine = calculate_cosine_distr(clusters)\n",
    "                            elbow_cosine_list.append(average_cosine)\n",
    "\n",
    "                        # CALCULATE COHERENCE  \n",
    "                        average_coherence = calculate_coherence(work_table, terms_only)\n",
    "                        elbow_coherence_list.append(average_coherence)\n",
    "    \n",
    "                        # CALCULATE EXCLUSIVITY\n",
    "                        average_exclusivity = calculate_exclusivity(terms_with_weight)\n",
    "                        elbow_exclusivity_list.append(average_exclusivity)    \n",
    "\n",
    "                    big_cosine_list.append(elbow_cosine_list)\n",
    "                    big_coherence_list.append(elbow_coherence_list)\n",
    "                    big_exclusivity_list.append(elbow_exclusivity_list) \n",
    "                    big_cosine_labels.append(model)\n",
    "                    big_coherence_labels.append(model)\n",
    "                    big_exclusivity_labels.append(model)  \n",
    "\n",
    "            # VISUALIZE COSINE SIMILARITY  \n",
    "            subplt = fig.add_subplot(spec[current_plot_row, 0])\n",
    "            for i in range(0, len(big_cosine_list)):\n",
    "                subplt.plot(\n",
    "                    cos_x_range,\n",
    "                    big_cosine_list[i],\n",
    "                    label = big_cosine_labels[i])\n",
    "            subplt.set_title('cosine sim. (min_df=' + str(min_df) + ', max_df=' + str(max_df) + ')')\n",
    "            subplt.legend(loc='best')\n",
    "\n",
    "            # VISUALIZE COHERENCE\n",
    "            subplt = fig.add_subplot(spec[current_plot_row, 1])\n",
    "            for j in range(0, len(big_coherence_list)):\n",
    "                subplt.plot(\n",
    "                    n_topics_range,\n",
    "                    big_coherence_list[j],\n",
    "                    label = big_coherence_labels[j])\n",
    "            subplt.set_title('coherence (min_df=' + str(min_df) + ', max_df=' + str(max_df) + ')')\n",
    "            subplt.legend(loc='best')\n",
    "\n",
    "            # VISUALIZE EXCLUSIVITY\n",
    "            subplt = fig.add_subplot(spec[current_plot_row, 2])\n",
    "            for k in range(0, len(big_exclusivity_list)):\n",
    "                subplt.plot(\n",
    "                    n_topics_range,\n",
    "                    big_exclusivity_list[k],\n",
    "                    label = big_exclusivity_labels[k])\n",
    "            subplt.set_title('exclusivity (min_df=' + str(min_df) + ', max_df=' + str(max_df) + ')')\n",
    "            subplt.legend(loc='best')\n",
    "\n",
    "            current_plot_row = current_plot_row + 1         \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTS THE NUMBER OF DOCUMENTS WITHOUT DOMINANT TOPIC\n",
    "def doc_without_dominant_topic_nmf(max_dfs, min_dfs, models, n_topic_min, n_topic_max, n_topic_step, folder):\n",
    "    \n",
    "    # INITIATE GRID PLOT (3X6)\n",
    "    num_cols = 1 # number of plot columns, equals to evaluation criteria (e.g. exclusivity) for topic models\n",
    "    num_rows = len(max_dfs)*len(min_dfs) # number of plot rows\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(num_cols*6.4, num_rows*4.8)) # add spacing and make it large (size in inches)\n",
    "    spec = fig.add_gridspec(ncols=num_cols, nrows=num_rows)\n",
    "    current_plot_row = 0\n",
    "\n",
    "    n_topics_range = range(n_topic_min, n_topic_max + 1, n_topic_step)\n",
    "    cos_x_range = list(n_topics_range)\n",
    "    while plot_x[0] < 3:\n",
    "        del plot_x[0]\n",
    "    \n",
    "    # EXECUTE, ONE PLOT DISPLAYS 4 MODELS ON TOP OF EACH OTHER\n",
    "    for max_df in max_dfs:\n",
    "        for min_df in min_dfs:\n",
    "        \n",
    "            big_no_dominant_list = [] \n",
    "            big_no_dominant_labels = []\n",
    "        \n",
    "            for model in models:\n",
    "\n",
    "                elbow_no_dominant_list = []\n",
    "\n",
    "                for n_topics in n_topics_range:\n",
    "            \n",
    "                    print('in current iteration, model=' + model + ', min_df=' + str(min_df) + ', max_df=' + str(max_df) + ', number of topics=' + str(n_topics))\n",
    "                    path = folder + model + '_min' + str(min_df) + '_max' + str(max_df) + '_k' + str(n_topics)\n",
    "\n",
    "                    work_table = extract_data(load_data(path))\n",
    "                    terms_only, terms_with_weight = extract_topics(path) \n",
    "                    clusters = create_clusters(work_table)\n",
    "   \n",
    "                    # CHECK IF THERE IS ANY DOC WITH TOPIC 0\n",
    "                    topic_zero_dominant = False\n",
    "    \n",
    "                    # CALCULATE THE NUMBER OF DOCUMENTS WITHOUT DOMINANT TOPIC\n",
    "                    if n_topics >= 3 :\n",
    "                        for cluster in clusters:\n",
    "                            if cluster[0] == '0':\n",
    "                                topic_zero_dominant = True\n",
    "                                nr_docs_without_dominant_topic = len(cluster[1]) \n",
    "                                elbow_no_dominant_list.append(nr_docs_without_dominant_topic)\n",
    "                \n",
    "                    if topic_zero_dominant == False:\n",
    "                        elbow_no_dominant_list.append(0)\n",
    "                \n",
    "                big_no_dominant_list.append(elbow_no_dominant_list) \n",
    "                big_no_dominant_labels.append(model)\n",
    "  \n",
    "            # VISUALIZE THE NUMBER OF DOCUMENTS WITHOUT DOMINANT TOPIC\n",
    "            subplt = fig.add_subplot(spec[current_plot_row, 0])\n",
    "            for l in range(0, len(big_no_dominant_list)):\n",
    "                subplt.plot(\n",
    "                    plot_x,\n",
    "                    big_no_dominant_list[l],\n",
    "                    label = big_no_dominant_labels[l])\n",
    "            subplt.set_title('no dominant topic (min_df=' + str(min_df) + ', max_df=' + str(max_df) + ')')\n",
    "            subplt.legend(loc='best')         \n",
    "\n",
    "            current_plot_row = current_plot_row + 1  \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTS THE NUMBER OF DOCUMENTS WITHOUT DOMINANT TOPIC\n",
    "def doc_without_dominant_topic_lda(max_dfs, min_dfs, alphas, betas, n_topic_min, n_topic_max, n_topic_step, folder):\n",
    "    \n",
    "    # INITIATE GRID PLOT (3X6)\n",
    "    num_cols = 1 # number of plot columns, equals to evaluation criteria (e.g. exclusivity) for topic models\n",
    "    num_rows = len(max_dfs) * len(min_dfs) # number of plot rows\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize = (num_cols * 6.4, num_rows * 4.8)) # add spacing and make it large (size in inches)\n",
    "    spec = fig.add_gridspec(ncols = num_cols, nrows = num_rows)\n",
    "    current_plot_row = 0\n",
    "\n",
    "    n_topics_range = range(n_topic_min, n_topic_max + 1, n_topic_step)   \n",
    "    \n",
    "    # EXECUTE, ONE PLOT DISPLAYS 4 MODELS ON TOP OF EACH OTHER\n",
    "    for max_df in max_dfs:\n",
    "        for min_df in min_dfs:\n",
    "        \n",
    "            big_no_dominant_list = [] \n",
    "            big_no_dominant_labels = []\n",
    "        \n",
    "            for alpha in alphas:\n",
    "                for beta in betas:\n",
    "                    model = 'a=' + str(alpha) + '/b=' + str(beta)\n",
    "\n",
    "                    elbow_no_dominant_list = []\n",
    "\n",
    "                    for n_topics in n_topics_range:\n",
    "            \n",
    "                        print('in current iteration, min_df=' + str(min_df) + ', max_df=' + str(max_df) + ', alpha=' + str(alpha) + ', beta=' + str(beta) + ', number of topics=' + str(n_topics))\n",
    "                        path = './IO_YO/TM_LDA_PERS/lda' + '_min' + str(min_df) + '_max' + str(max_df) + '_alpha' + str(alpha) + '_beta' + str(beta) + '_k' + str(n_topics)     \n",
    "\n",
    "                        work_table = extract_data(load_data(path))\n",
    "                        terms_only, terms_with_weight = extract_topics(path) \n",
    "                        clusters = create_clusters(work_table)\n",
    "   \n",
    "                        # CHECK IF THERE IS ANY DOC WITH TOPIC 0\n",
    "                        topic_zero_dominant = False\n",
    "    \n",
    "                        # CALCULATE THE NUMBER OF DOCUMENTS WITHOUT DOMINANT TOPIC\n",
    "                        if n_topics >= 3 :\n",
    "                            for cluster in clusters:\n",
    "                                if cluster[0] == '0':\n",
    "                                    topic_zero_dominant = True\n",
    "                                    nr_docs_without_dominant_topic = len(cluster[1]) \n",
    "                                    elbow_no_dominant_list.append(nr_docs_without_dominant_topic)\n",
    "                \n",
    "                        if topic_zero_dominant == False:\n",
    "                            elbow_no_dominant_list.append(0)\n",
    "\n",
    "                    big_no_dominant_list.append(elbow_no_dominant_list) \n",
    "                    big_no_dominant_labels.append(model)\n",
    "\n",
    "\n",
    "            # VISUALIZE THE NUMBER OF DOCUMENTS WITHOUT DOMINANT TOPIC\n",
    "            subplt = fig.add_subplot(spec[current_plot_row, 0])\n",
    "            for l in range(0, len(big_no_dominant_list)):\n",
    "                subplt.plot(\n",
    "                    n_topics_range,\n",
    "                    big_no_dominant_list[l],\n",
    "                    label = big_no_dominant_labels[l])\n",
    "            subplt.set_title('no dominant topic (min_df=' + str(min_df) + ', max_df=' + str(max_df) + ')')\n",
    "            subplt.legend(loc='best')         \n",
    "\n",
    "            current_plot_row = current_plot_row + 1 \n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT BEST VARIANTS, SUITABLE FOR MIXED NMF AND LDA\n",
    "def evaluate_bestof(max_dfs, min_dfs, models, n_topic_min, n_topic_max, n_topic_step, cosine_basis): \n",
    "    \n",
    "    if cosine_basis == True:\n",
    "        # READ VOCABULARY\n",
    "        vocabulary = read_vocab()\n",
    "\n",
    "    # INITIATE GRID PLOT (3X6)\n",
    "    num_cols = 3 # number of plot columns, equals to evaluation criteria (e.g. exclusivity) for topic models\n",
    "    num_rows = len(max_dfs) * len(min_dfs) # number of plot rows\n",
    "\n",
    "    fig = plt.figure(constrained_layout = True, figsize = (num_cols * 6.4, num_rows * 4.8)) # add spacing and make it large (size in inches)\n",
    "    spec = fig.add_gridspec(ncols = num_cols, nrows = num_rows)\n",
    "    current_plot_row = 0\n",
    "    \n",
    "    for max_df in max_dfs:\n",
    "        for min_df in min_dfs:\n",
    "\n",
    "            big_cosine_list = []\n",
    "            big_coherence_list = []\n",
    "            big_exclusivity_list = []\n",
    "            big_no_dominant_list = []\n",
    "            big_cosine_labels = []     \n",
    "            big_coherence_labels = []   \n",
    "            big_exclusivity_labels = [] \n",
    "            big_no_dominant_labels = []\n",
    "\n",
    "            for model in models:\n",
    "            \n",
    "                folder = './IO_YO/' + model[0] + '/'\n",
    "                filename_prefix = model[1]\n",
    "                alpha = model[2]\n",
    "                beta = model[3]\n",
    "                label = model[4]\n",
    "            \n",
    "                if filename_prefix == 'lda':\n",
    "                    filename_ab_part = '_alpha' + alpha + '_beta' + beta\n",
    "                else:\n",
    "                    filename_ab_part = '' # no alpha or beta for NMF models\n",
    "\n",
    "                elbow_cosine_list = []     \n",
    "                elbow_coherence_list = []   \n",
    "                elbow_exclusivity_list = [] \n",
    "                elbow_no_dominant_list = []\n",
    "\n",
    "                for n_topics in range(n_topic_min, n_topic_max + 1):\n",
    "\n",
    "                    filename_stem = filename_prefix + '_min' + str(min_df) + '_max' + str(max_df) + filename_ab_part + '_k' + str(n_topics)\n",
    "                    print('in current iteration, ' + filename_stem)\n",
    "                    path = folder + filename_stem\n",
    "\n",
    "                    work_table = extract_data(load_data(path))\n",
    "                    terms_only, terms_with_weight = extract_topics(path) \n",
    "                    clusters = create_clusters(work_table)\n",
    "\n",
    "                    average_cosine = None\n",
    "                    # CALCULATE COSINE SIMILARITY\n",
    "                    if n_topics >= 2 :\n",
    "                        if cosine_basis == True:\n",
    "                            average_cosine = calculate_cosine_doc(clusters, vocabulary)  \n",
    "                        else:\n",
    "                            average_cosine = calculate_cosine_distr(clusters)\n",
    "                        elbow_cosine_list.append(average_cosine)\n",
    "\n",
    "                    # CALCULATE COHERENCE  \n",
    "                    average_coherence = calculate_coherence(work_table, terms_only)\n",
    "                    elbow_coherence_list.append(average_coherence)\n",
    "\n",
    "                    # CALCULATE EXCLUSIVITY\n",
    "                    average_exclusivity = calculate_exclusivity(terms_with_weight)\n",
    "                    elbow_exclusivity_list.append(average_exclusivity)    \n",
    "\n",
    "                    # CALCULATE THE NUMBER OF DOCUMENTS WITHOUT DOMINANT TOPIC\n",
    "                    if n_topics >= 3 :\n",
    "                        for cluster in clusters:\n",
    "                            if cluster[0] == '0':\n",
    "                                nr_docs_without_dominant_topic = len(cluster[1]) \n",
    "                                elbow_no_dominant_list.append(nr_docs_without_dominant_topic)\n",
    "\n",
    "\n",
    "                big_cosine_list.append(elbow_cosine_list)\n",
    "                big_coherence_list.append(elbow_coherence_list)\n",
    "                big_exclusivity_list.append(elbow_exclusivity_list) \n",
    "                big_no_dominant_list.append(elbow_no_dominant_list)\n",
    "                big_cosine_labels.append(label)\n",
    "                big_coherence_labels.append(label)\n",
    "                big_exclusivity_labels.append(label)  \n",
    "                big_no_dominant_labels.append(label)\n",
    "\n",
    "            # VISUALIZE COSINE SIMILARITY  \n",
    "            subplt = fig.add_subplot(spec[current_plot_row, 0])         \n",
    "            for i in range(0, len(big_cosine_list)):\n",
    "                subplt.plot(\n",
    "                    range(max(n_topic_min, 2), n_topic_max+1),\n",
    "                    big_cosine_list[i],\n",
    "                    label = big_cosine_labels[i])\n",
    "            subplt.set_title('cosine sim. (min_df=' + str(min_df) + ', max_df=' + str(max_df) + ')')\n",
    "            subplt.legend(loc='best')\n",
    "\n",
    "            # VISUALIZE COHERENCE\n",
    "            subplt = fig.add_subplot(spec[current_plot_row, 1])\n",
    "            for j in range(0, len(big_coherence_list)):\n",
    "                subplt.plot(\n",
    "                    range(n_topic_min, n_topic_max+1),\n",
    "                    big_coherence_list[j],\n",
    "                    label = big_coherence_labels[j])\n",
    "            subplt.set_title('coherence (min_df=' + str(min_df) + ', max_df=' + str(max_df) + ')')\n",
    "            subplt.legend(loc='best')\n",
    "\n",
    "            # VISUALIZE EXCLUSIVITY\n",
    "            subplt = fig.add_subplot(spec[current_plot_row, 2])\n",
    "            for k in range(0, len(big_exclusivity_list)):\n",
    "                subplt.plot(\n",
    "                    range(n_topic_min, n_topic_max+1),\n",
    "                    big_exclusivity_list[k],\n",
    "                    label = big_exclusivity_labels[k])\n",
    "            subplt.set_title('exclusivity (min_df=' + str(min_df) + ', max_df=' + str(max_df) + ')')\n",
    "            subplt.legend(loc='best')\n",
    "\n",
    "            current_plot_row = current_plot_row + 1   \n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in current iteration, model=NMF_TFIDF_FR, min_df=1, max_df=60, number of topics=1\n",
      "in current iteration, model=NMF_TFIDF_FR, min_df=1, max_df=60, number of topics=2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-4855d96e66ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./IO_YO/TM_NMF_P/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'NMF_TFIDF_FR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NMF_TFIDF_KL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NMF_TF_FR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NMF_TF_KL'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mevaluate_nmf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_dfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_dfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_topic_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_topic_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_topic_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosine_basis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m#doc_without_dominant_topic_nmf(max_dfs, min_dfs, models, n_topic_min, n_topic_max, n_topic_step, folder)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-df01c29db505>\u001b[0m in \u001b[0;36mevaluate_nmf\u001b[1;34m(max_dfs, min_dfs, models, n_topic_min, n_topic_max, n_topic_step, folder, cosine_basis)\u001b[0m\n\u001b[0;32m     47\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mn_topics\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mcosine_basis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                             \u001b[0maverage_cosine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_cosine_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                             \u001b[0maverage_cosine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_cosine_distr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-dac4fba8d2e9>\u001b[0m in \u001b[0;36mcalculate_cosine_doc\u001b[1;34m(clusters, vocabulary)\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mword_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                     \u001b[0mword_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1382.4x2073.6 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# One doc per person\n",
    "#max_dfs = [120, 90, 70]\n",
    "max_dfs = [60, 51, 45]\n",
    "min_dfs = [1, 2]\n",
    "n_topic_min = 1\n",
    "n_topic_max = 40\n",
    "n_topic_step = 1       # One doc per transaction\n",
    "cosine_basis = True    # True = document vector, False = topic ditribution vector\n",
    "#----------------------------------------------\n",
    "\n",
    "# EXECUTE NMF\n",
    "folder = './IO_YO/TM_NMF_P/'\n",
    "models = ['NMF_TFIDF_FR', 'NMF_TFIDF_KL', 'NMF_TF_FR', 'NMF_TF_KL']\n",
    "evaluate_nmf(max_dfs, min_dfs, models, n_topic_min, n_topic_max, n_topic_step, folder, cosine_basis)\n",
    "#doc_without_dominant_topic_nmf(max_dfs, min_dfs, models, n_topic_min, n_topic_max, n_topic_step, folder)\n",
    "\n",
    "#EXECUTE LDA\n",
    "alphas = [None, 1.0]\n",
    "betas = [None, 1.0]\n",
    "folder = './IO_YO/TM_LDA_P/'\n",
    "#evaluate_lda(max_dfs, min_dfs, alphas, betas, n_topic_min, n_topic_max, n_topic_step, folder, cosine_basis)\n",
    "#doc_without_dominant_topic_lda(max_dfs, min_dfs, alphas, betas, n_topic_min, n_topic_max, n_topic_step, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNERS\n",
    "#max_dfs = [120, 90, 70]\n",
    "max_dfs = [60, 51, 45]    \n",
    "min_dfs = [1, 2]\n",
    "cosine_basis = False\n",
    "n_topic_min = 1\n",
    "n_topic_max = 40\n",
    "n_topic_step = 1 # One doc per transaction\n",
    "\n",
    "#            folder      model variant     alpha (only LDA)   beta (only LDA)      plot label\n",
    "\"\"\"\n",
    "models = [['TM_LDA_P',   'lda',               'None',             'None',         'LDA a=1/k, b=1/k'],\n",
    "          ['TM_LDA_P',   'lda',               'None',              '1.0',         'LDA a=1/k, b=1.0'],\n",
    "          ['TM_NMF_P',   'nmf_tfidf_fr',       None,               None,          'NMF TFIDF FR'      ],\n",
    "          ['TM_NMF_P',   'nmf_tfidf_kl',       None,               None,          'NMF TFIDF KL'      ]]\n",
    "\"\"\"\n",
    "models = [['TM_LDA_P',   'lda',               'None',             'None',         'LDA a=1/k, b=1/k'],\n",
    "          ['TM_NMF_P',   'nmf_tfidf_kl',       None,               None,          'NMF TFIDF KL'      ]]\n",
    "#----------------------------------------------\n",
    "\n",
    "evaluate_bestof(max_dfs, min_dfs, models, n_topic_min, n_topic_max, n_topic_step, cosine_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
